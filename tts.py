import os
import json
import subprocess
import threading
import time
from openai import OpenAI
from dotenv import load_dotenv
import cv2
import cv2.aruco as aruco

# ============================================
# 0. ê³µí†µ ì„¤ì •
# ============================================
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

TTS_MODEL = "gpt-4o-mini-tts"   # ìŒì„± ìƒì„± ëª¨ë¸
TEXT_MODEL = "gpt-4o-mini"      # ëŒ€ì‚¬ ìƒì„± ëª¨ë¸

ALLOWED_VOICES = {
    "alloy", "echo", "fable", "onyx", "nova", "shimmer",
    "coral", "verse", "ballad", "ash", "sage", "marin", "cedar"
}

# í˜„ì¬ ìƒíƒœ (ë°°ê²½ / ìºë¦­í„°)
CURRENT_BG_BOOK_CODE = None      # í˜„ì¬ ë°°ê²½ì´ ëœ ì±… ì½”ë“œ
CURRENT_BG_INFO = None           # í˜„ì¬ ë°°ê²½ ì •ë³´
CURRENT_CHA1_INFO = None         # í˜„ì¬ cha1 ìºë¦­í„° dict
CURRENT_CHA2_INFO = None         # í˜„ì¬ cha2 ìºë¦­í„° dict

# ë¹„ë””ì˜¤ í”Œë ˆì´ì–´ (ìŠ¤ë ˆë“œ ê¸°ë°˜)
class VideoPlayer:
    """OpenCV ê¸°ë°˜ ë¹„ë””ì˜¤ í”Œë ˆì´ì–´ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ë¬´í•œ ë£¨í”„ ì¬ìƒ, ì˜¤ë””ì˜¤ í¬í•¨)"""
    
    def __init__(self):
        self.current_video_path = None
        self.video_cap = None
        self.next_video_path = None
        self.frame = None
        self.lock = threading.Lock()
        self.running = False
        self.thread = None
        self.audio_process = None  # ì˜¤ë””ì˜¤ ì¬ìƒ í”„ë¡œì„¸ìŠ¤
        self.fade_alpha = 1.0  # í˜ì´ë“œ ì•ŒíŒŒ ê°’ (0.0 ~ 1.0)
        self.is_fading = False  # í˜ì´ë“œ ì¤‘ì¸ì§€ ì—¬ë¶€
        self.fade_duration = 0.5  # í˜ì´ë“œ ì§€ì† ì‹œê°„ (ì´ˆ)
        self.fade_start_time = None
    
    def _play_loop(self):
        """ë¹„ë””ì˜¤ ì¬ìƒ ë£¨í”„ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
        while self.running:
            with self.lock:
                # í˜ì´ë“œ íš¨ê³¼ ì²˜ë¦¬
                if self.is_fading and self.fade_start_time:
                    elapsed = time.time() - self.fade_start_time
                    if elapsed < self.fade_duration:
                        # í˜ì´ë“œì•„ì›ƒ: 1.0 -> 0.0
                        self.fade_alpha = 1.0 - (elapsed / self.fade_duration)
                    elif elapsed < self.fade_duration * 2:
                        # ë¹„ë””ì˜¤ ì „í™˜
                        if self.next_video_path and self.next_video_path != self.current_video_path:
                            self._switch_video_internal(self.next_video_path)
                            self.next_video_path = None
                        # í˜ì´ë“œì¸: 0.0 -> 1.0
                        self.fade_alpha = (elapsed - self.fade_duration) / self.fade_duration
                    else:
                        # í˜ì´ë“œ ì™„ë£Œ
                        self.is_fading = False
                        self.fade_alpha = 1.0
                        self.fade_start_time = None
                
                if self.video_cap is None or not self.video_cap.isOpened():
                    time.sleep(0.01)
                    continue
                
                ret, frame = self.video_cap.read()
                if not ret:
                    # ë¹„ë””ì˜¤ ëë‚˜ë©´ ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°€ê¸° (ë¬´í•œ ë£¨í”„)
                    self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                    ret, frame = self.video_cap.read()
                
                if ret:
                    # í˜ì´ë“œ íš¨ê³¼ ì ìš©
                    if self.is_fading and self.fade_alpha < 1.0:
                        # ê²€ì€ í™”ë©´ê³¼ ë¸”ë Œë”©
                        black_frame = frame.copy()
                        black_frame.fill(0)
                        frame = cv2.addWeighted(frame, self.fade_alpha, black_frame, 1.0 - self.fade_alpha, 0)
                    self.frame = frame
            
            # í”„ë ˆì„ ë ˆì´íŠ¸ ë§ì¶”ê¸° (ì•½ 30fps)
            time.sleep(0.033)
    
    def _switch_video_internal(self, video_path: str):
        """ë‚´ë¶€ ë¹„ë””ì˜¤ ì „í™˜ (í˜ì´ë“œ ì¤‘ì— í˜¸ì¶œ)"""
        # ê¸°ì¡´ ë¹„ë””ì˜¤ í•´ì œ
        if self.video_cap:
            self.video_cap.release()
        
        # ìƒˆ ë¹„ë””ì˜¤ ì—´ê¸°
        self.current_video_path = video_path
        self.video_cap = cv2.VideoCapture(video_path)
        if not self.video_cap.isOpened():
            print(f"âŒ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŒ: {video_path}")
            self.video_cap = None
        else:
            print(f"ğŸ¬ ë¹„ë””ì˜¤ ì „í™˜: {video_path}")
            
            # ì˜¤ë””ì˜¤ ì¬ìƒ ì‹œì‘ (ë¬´í•œ ë£¨í”„)
            self._start_audio(video_path)
    
    def _start_audio(self, video_path: str):
        """ë¹„ë””ì˜¤ì˜ ì˜¤ë””ì˜¤ë¥¼ ë¬´í•œ ë£¨í”„ë¡œ ì¬ìƒ"""
        # ê¸°ì¡´ ì˜¤ë””ì˜¤ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
        if self.audio_process:
            # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (macOS ìŠ¤ë ˆë“œ)
            if isinstance(self.audio_process, dict):
                self.audio_process["ref"]["running"] = False
            # í”„ë¡œì„¸ìŠ¤ì¸ ê²½ìš°
            elif hasattr(self.audio_process, "terminate"):
                try:
                    self.audio_process.terminate()
                    self.audio_process.wait(timeout=1.0)
                except:
                    try:
                        self.audio_process.kill()
                    except:
                        pass
            self.audio_process = None
        
        # ffmpegë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ì˜ ì˜¤ë””ì˜¤ë¥¼ ë¬´í•œ ë£¨í”„ë¡œ ì¬ìƒ
        import platform
        is_macos = platform.system() == "Darwin"
        
        try:
            if is_macos:
                # macOS: ffmpegë¡œ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•˜ê³  afplayë¡œ ì¬ìƒ (ë¬´í•œ ë£¨í”„)
                # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ë¬´í•œ ë£¨í”„ë¡œ ì¬ìƒ
                audio_thread_ref = {"running": True}
                
                def play_audio_loop():
                    while audio_thread_ref["running"]:
                        try:
                            # ffmpegë¡œ ì˜¤ë””ì˜¤ë¥¼ ë¬´í•œ ë£¨í”„ë¡œ ì¬ìƒ (stream_loop ì‚¬ìš©)
                            proc = subprocess.Popen(
                                ["ffmpeg", "-re", "-stream_loop", "-1", "-i", video_path, 
                                 "-vn", "-acodec", "pcm_s16le", "-ar", "44100", "-ac", "2", "-f", "wav", "-"],
                                stdout=subprocess.PIPE,
                                stderr=subprocess.DEVNULL,
                                bufsize=0
                            )
                            # afplayë¡œ ì¬ìƒ (stdinì—ì„œ ì½ê¸°)
                            afplay_proc = subprocess.Popen(
                                ["afplay", "-"],
                                stdin=proc.stdout,
                                stdout=subprocess.DEVNULL,
                                stderr=subprocess.DEVNULL
                            )
                            afplay_proc.wait()
                            proc.wait()
                        except Exception as e:
                            print(f"âš ï¸ ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜: {e}")
                            break
                
                # ì˜¤ë””ì˜¤ ì¬ìƒ ìŠ¤ë ˆë“œ ì‹œì‘
                audio_thread = threading.Thread(target=play_audio_loop, daemon=True)
                audio_thread.start()
                self.audio_process = {"thread": audio_thread, "ref": audio_thread_ref}
            else:
                # Linux: ffplay ì‚¬ìš©
                self.audio_process = subprocess.Popen(
                    ["ffplay", "-nodisp", "-autoexit", "-loop", "0", "-loglevel", "quiet", video_path],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL
                )
        except FileNotFoundError:
            print("âš ï¸ ffmpeg/ffplayë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë””ì˜¤ëŠ” ì¬ìƒë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            if is_macos:
                print("   macOSì—ì„œëŠ” 'brew install ffmpeg'ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
            else:
                print("   Linuxì—ì„œëŠ” 'sudo apt-get install ffmpeg' ë˜ëŠ” 'sudo yum install ffmpeg'ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
    
    def start(self):
        """í”Œë ˆì´ì–´ ì‹œì‘"""
        if not self.running:
            self.running = True
            self.thread = threading.Thread(target=self._play_loop, daemon=True)
            self.thread.start()
    
    def stop(self):
        """í”Œë ˆì´ì–´ ì¤‘ì§€"""
        self.running = False
        if self.thread:
            self.thread.join(timeout=1.0)
        with self.lock:
            if self.video_cap:
                self.video_cap.release()
                self.video_cap = None
            self.frame = None
        # ì˜¤ë””ì˜¤ í”„ë¡œì„¸ìŠ¤/ìŠ¤ë ˆë“œ ì¢…ë£Œ
        if self.audio_process:
            # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (macOS ìŠ¤ë ˆë“œ)
            if isinstance(self.audio_process, dict):
                self.audio_process["ref"]["running"] = False
            # í”„ë¡œì„¸ìŠ¤ì¸ ê²½ìš° (Linux)
            elif hasattr(self.audio_process, "terminate"):
                try:
                    self.audio_process.terminate()
                    self.audio_process.wait(timeout=1.0)
                except:
                    try:
                        self.audio_process.kill()
                    except:
                        pass
            self.audio_process = None
    
    def set_video(self, video_path: str):
        """ë¹„ë””ì˜¤ íŒŒì¼ ë³€ê²½ (í˜ì´ë“œ íš¨ê³¼ì™€ í•¨ê»˜ ë¶€ë“œëŸ¬ìš´ ì „í™˜)"""
        with self.lock:
            if self.current_video_path is None:
                # ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ëŠ” í˜ì´ë“œ ì—†ì´ ë°”ë¡œ ì‹œì‘
                self._switch_video_internal(video_path)
            else:
                # ë‹¤ìŒ ë¹„ë””ì˜¤ë¡œ ì „í™˜ (í˜ì´ë“œ íš¨ê³¼)
                self.next_video_path = video_path
                self.is_fading = True
                self.fade_start_time = time.time()
    
    def get_frame(self):
        """í˜„ì¬ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°"""
        with self.lock:
            if self.frame is not None:
                return self.frame.copy()
        return None

# ì „ì—­ ë¹„ë””ì˜¤ í”Œë ˆì´ì–´ ì¸ìŠ¤í„´ìŠ¤
VIDEO_PLAYER = VideoPlayer()

# ë°°ê²½ ë¹„ë””ì˜¤ ì„¤ì •
BG_VIDEO_DIR = "bg_video"
BOOK_TO_VIDEO = {
    "BJBJ": "10_BJBJ_matchedSize.mov",
    "PSJ": "11_PSJ_matchedSize.mov",
    "DGJ": "13_DGJ_matchedSize.mov",
    "HBJ": "17_HBJ_matchedSize.mov",
    "JWCJ": "19_JWCJ_matchedSize.mov",
    "KWJ": "3_KWJ_matchedSize.mov",
    "OGJJ": "5_OGJJ_matchedSize.mov",
    "JHHRJ": "6_JHHRJ_matchedSize.mov",
    "SCJ": "7_SCJ_matchedSize.mov",
}


# ============================================
# 1. ì„¤ì • íŒŒì¼ ë¡œë“œ
# ============================================
def load_json(path: str):
    if not os.path.exists(path):
        raise FileNotFoundError(f"{path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

CHARACTERS = load_json("characters.json")
BACKGROUNDS = load_json("backgrounds.json")

# ì±… ì½”ë“œ â†’ cha1 / cha2 ì—­í•  í‚¤ ë§¤í•‘
ROLE_MAP = {
    "SCJ": {"cha1": "simcheong",    "cha2": "simbongsa"},
    "HBJ": {"cha1": "heungbu",      "cha2": "nolbu"},
    "BJBJ": {"cha1": "turtle",      "cha2": "rabbit"},
    "OGJJ": {"cha1": "onggojip",    "cha2": "onggojip"},
    "JWCJ": {"cha1": "jeonwoochi",  "cha2": "jeonwoochi"},
    "JHHRJ": {"cha1": "sister_older",    "cha2": "ghost"},
    "PSJ": {"cha1": "ugly",         "cha2": "pretty"},
    "DGJ": {"cha1": "toad",         "cha2": "fox"},
    "KWJ": {"cha1": "kimwon",       "cha2": "monster"}
}

# ============================================
# ArUco ë§ˆì»¤ ì„¤ì •
# ============================================
ARUCO_DICTIONARY = aruco.getPredefinedDictionary(aruco.DICT_5X5_1000)
ARUCO_MARKER_SIZE = 200  # pixels

# ë§ˆì»¤ ID â†’ ì±… ì½”ë“œ ë§¤í•‘
MARKER_TO_BOOK = {
    1: "KWJ",      # 03_KWJ
    2: "PSJ",      # 11_PSJ
    6: "DGJ",      # 13_DGJ
    7: "JHHRJ",    # 06_JHHRJ
    8: "JWCJ",     # 19_JWCJ
    9: "HBJ",      # 17_HBJ
    10: "OGJJ",    # 05_OGJJ
    11: "SCJ",     # 07_SCJ
    12: "BJBJ",    # 10_BJBJ
}

# ë§ˆì»¤ ID â†’ íŒŒì¼ëª… ë§¤í•‘
MARKER_NAMES = {
    1: "03_KWJ",
    2: "11_PSJ",
    6: "13_DGJ",
    7: "06_JHHRJ",
    8: "19_JWCJ",
    9: "17_HBJ",
    10: "05_OGJJ",
    11: "07_SCJ",
    12: "10_BJBJ",
}


def generate_aruco_markers(output_dir: str = "markers"):
    """
    ArUco ë§ˆì»¤ ì´ë¯¸ì§€ë“¤ì„ ìƒì„±í•˜ì—¬ ì§€ì •ëœ í´ë”ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    os.makedirs(output_dir, exist_ok=True)
    
    for marker_id, name in MARKER_NAMES.items():
        marker_image = aruco.generateImageMarker(ARUCO_DICTIONARY, marker_id, ARUCO_MARKER_SIZE)
        filename = os.path.join(output_dir, f"{name}.png")
        cv2.imwrite(filename, marker_image)
        print(f"âœ… Saved ArUco marker: {filename}")
    
    print(f"\nğŸ¯ ì´ {len(MARKER_NAMES)}ê°œì˜ ArUco ë§ˆì»¤ê°€ '{output_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def get_book_code_from_marker(marker_id: int) -> str | None:
    """
    ArUco ë§ˆì»¤ IDë¡œë¶€í„° ì±… ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    return MARKER_TO_BOOK.get(marker_id)

# ============================================
# 2. Background ê´€ë ¨
# ============================================
def get_background(book_code: str):
    return BACKGROUNDS.get(book_code)

# ë°°ê²½ ìŒì•…ì€ ì´ì œ ë¹„ë””ì˜¤ì— í¬í•¨ëœ ì˜¤ë””ì˜¤ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ í•¨ìˆ˜ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
# def play_background_music_if_exists(bg_info: dict):
#     pass


def stop_background_video():
    """í˜„ì¬ ì¬ìƒ ì¤‘ì¸ ë°°ê²½ ë¹„ë””ì˜¤ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤."""
    VIDEO_PLAYER.stop()
    print("ğŸ¬ ë°°ê²½ ë¹„ë””ì˜¤ ì¤‘ì§€ë¨")


def play_background_video(book_code: str):
    """
    ì±… ì½”ë“œì— í•´ë‹¹í•˜ëŠ” ë°°ê²½ ë¹„ë””ì˜¤ë¥¼ ë¬´í•œ ë£¨í”„ë¡œ ì¬ìƒí•©ë‹ˆë‹¤.
    ê°™ì€ ìœˆë„ìš°ì—ì„œ ë¶€ë“œëŸ½ê²Œ ì „í™˜ë©ë‹ˆë‹¤.
    """
    # ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ í™•ì¸
    video_file = BOOK_TO_VIDEO.get(book_code)
    if video_file is None:
        print(f"ğŸ¬ '{book_code}'ì— í•´ë‹¹í•˜ëŠ” ë°°ê²½ ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    video_path = os.path.join(BG_VIDEO_DIR, video_file)
    if not os.path.exists(video_path):
        print(f"ğŸ¬ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {video_path}")
        return
    
    # VideoPlayerë¥¼ í†µí•´ ë¹„ë””ì˜¤ ì „í™˜ (ê°™ì€ ìœˆë„ìš°ì—ì„œ ë¶€ë“œëŸ½ê²Œ)
    VIDEO_PLAYER.set_video(video_path)

def get_interaction_profile(bg_info: dict) -> dict:
    """
    backgrounds.json ì•ˆì— ë¯¸ë¦¬ ì •ì˜í•´ ë‘”
    interaction_label / interaction_summary / interaction_emotionsë¥¼ ê°€ì ¸ì˜¨ë‹¤.
    interaction_emotionsëŠ” 10ê°€ì§€ ê°ì • ì˜µì…˜ ë¦¬ìŠ¤íŠ¸ë¡œ, LLMì´ ìºë¦­í„° ì„±ê²©ì— ë§ê²Œ ì„ íƒí•œë‹¤.
    """
    if bg_info is None:
        return {
            "label": "neutral",
            "summary": "A neutral situation with no special context",
            "emotion_options": ["mild curiosity", "calm observation", "quiet interest"]
        }

    # interaction_emotionsëŠ” ì´ì œ ë¦¬ìŠ¤íŠ¸
    emotions_data = bg_info.get("interaction_emotions", [])
    if isinstance(emotions_data, list):
        emotion_options = emotions_data
    else:
        # í˜¹ì‹œ ë¬¸ìì—´ì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        emotion_options = [emotions_data]

    return {
        "label": bg_info.get("interaction_label", "neutral"),
        "summary": bg_info.get(
            "interaction_summary",
            f"A scene involving '{bg_info.get('interaction', '')}'"
        ),
        "emotion_options": emotion_options
    }

# ============================================
# 3. Character ê´€ë ¨
# ============================================
def build_character(book_code: str, role_key: str) -> dict:
    data = CHARACTERS[book_code][role_key]

    gender = data["gender"]
    age = data["age"]
    base_desc = data["base_personality"]
    raw_voice = data.get("voice", "alloy")
    speed = data.get("speed", 1.0)

    voice = raw_voice if raw_voice in ALLOWED_VOICES else "alloy"

    personality = base_desc

    return {
        "book_code": book_code,
        "role_key": role_key,
        "gender": gender,
        "age": age,
        "voice": voice,
        "personality": personality,
        "speed": speed,
    }

def build_sisters_pair() -> tuple[dict, dict]:
    older = build_character("JHHRJ", "sister_older")
    younger = build_character("JHHRJ", "sister_younger")
    return older, younger

# ============================================
# 4. í…ìŠ¤íŠ¸ LLMìœ¼ë¡œ ëŒ€ì‚¬ ìƒì„±
# ============================================
def _clean_line(text: str) -> str:
    if not text:
        return ""
    line = text.strip().splitlines()[0]
    line = line.strip().strip("ã€Œã€\"'â€œâ€â€˜â€™")
    return line


def generate_action_line(character: dict, bg_info: dict) -> str:
    """
    ë°°ê²½/ì¸í„°ë™ì…˜ì„ ë³´ê³  ìºë¦­í„°ê°€ ê·¸ í–‰ë™ì„ í•˜ê¸° ì§ì „ì— í•˜ëŠ” í•œ ë§ˆë””.
    â†’ ìµœëŒ€í•œ ì§§ê³  êµ¬ì–´ì²´, ì‚¬ëŒ ë§ì²˜ëŸ¼.
    """
    place = bg_info.get("background", "")
    action = bg_info.get("interaction", "")
    profile = get_interaction_profile(bg_info)
    emotion_list = "\n".join([f"  - {e}" for e in profile['emotion_options']])

    # ìºë¦­í„°ì˜ speech_patterns ê°€ì ¸ì˜¤ê¸°
    char_data = CHARACTERS.get(character['book_code'], {}).get(character['role_key'], {})
    speech_patterns = char_data.get('speech_patterns', {})
    speaking_style = speech_patterns.get('speaking_style', '')

    system = (
        "ë‹¹ì‹ ì€ í•œêµ­ ì˜›ì´ì•¼ê¸° ì† ë“±ì¥ì¸ë¬¼ì´ ì‹¤ì œë¡œ ë§í•˜ëŠ” ëŒ€ì‚¬ë¥¼ ì“°ëŠ” ì‘ê°€ì…ë‹ˆë‹¤. "
        "ëŒ€ë³¸ì´ë‚˜ ë‚˜ë ˆì´ì…˜ì´ ì•„ë‹ˆë¼, ì‚¬ëŒì´ ì…ìœ¼ë¡œ íˆ­ íŠ€ì–´ë‚˜ì˜¤ê²Œ ë§í•˜ëŠ” í•œêµ­ì–´ êµ¬ì–´ì²´ë¥¼ ë§Œë“œì„¸ìš”."
    )

    user = f"""
ë°°ê²½ ì¥ì†Œ: {place}
ë°°ê²½ ì¸í„°ë™ì…˜: {action}

ì¥ë©´ ë¶„ìœ„ê¸°:
- ìš”ì•½: {profile['summary']}
- ê°€ëŠ¥í•œ ê°ì •ë“¤ (ìºë¦­í„° ì„±ê²©ì— ë§ëŠ” ê²ƒì„ ì„ íƒí•˜ì„¸ìš”):
{emotion_list}

ìºë¦­í„° ì„¤ì •(ì˜ì–´): {character['personality']}
ìºë¦­í„° ì •ë³´: {character['age']}ì‚´ {character['gender']}
ìºë¦­í„° ë§íˆ¬ ìŠ¤íƒ€ì¼: {speaking_style}

ìƒí™©:
- ì´ ìºë¦­í„°ê°€ ì§€ê¸ˆ '{action}'ì„(ë¥¼) í•˜ê¸° ì§ì „ì…ë‹ˆë‹¤.
- ìœ„ ê°ì • ì˜µì…˜ ì¤‘ ì´ ìºë¦­í„°ì˜ ì„±ê²©ì— ê°€ì¥ ì–´ìš¸ë¦¬ëŠ” ê°ì •ì„ ì„ íƒí•˜ê³ , ê·¸ ê°ì •ì„ ë‹´ì•„ ì§§ê²Œ í•œ ë§ˆë””ë¥¼ í•©ë‹ˆë‹¤.

ë§íˆ¬ ê·œì¹™:
- ë¬¸ì–´ì²´(ì˜ˆ: '~ê²ƒì´ë‹¤', '~í•©ë‹ˆë‹¤')ë¥¼ ì ˆëŒ€ ì“°ì§€ ë§ˆì„¸ìš”.
- ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ë§Œ ì“°ì„¸ìš”. (ì˜ˆ: '~ê±°ì•¼', '~í•˜ëŠ” ê±´ê°€?', '~í•´ë³¼ê¹Œ', '~í•˜ë„¤ìš”' ë“±)
- ìºë¦­í„°ì˜ ë§íˆ¬ ìŠ¤íƒ€ì¼ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”. íŠ¹íˆ '~ì´ê¸°ì•¼' ê°™ì€ ë¹„ë¬¸ë²•ì  í‘œí˜„ì„ ì“°ì§€ ë§ê³  '~ì´ì§€' ê°™ì€ ì˜¬ë°”ë¥¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- ë„ˆë¬´ ê¸¸ê²Œ ì„¤ëª…í•˜ì§€ ë§ê³ , 1~2ì´ˆ ì•ˆì— ë§í•  ìˆ˜ ìˆì„ ì •ë„ì˜ ì§§ì€ í•œ ë¬¸ì¥ìœ¼ë¡œ.
- ëŠë‚Œí‘œë‚˜ ë¬¼ìŒí‘œëŠ” ì¨ë„ ë˜ì§€ë§Œ, ë¬¸ì¥ì€ í•˜ë‚˜ë§Œ.
- ë”°ì˜´í‘œ( ", ã€ ã€ ë“±)ëŠ” ì“°ì§€ ë§ˆì„¸ìš”.

ì¶œë ¥:
- ì¡°ê±´ì„ ì§€í‚¤ëŠ” í•œêµ­ì–´ í•œ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

    resp = client.responses.create(
        model=TEXT_MODEL,
        input=[
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ],
        max_output_tokens=50,
        temperature=0.7  # ë„ˆë¬´ íŠ€ì§€ ì•Šê²Œ ì•½ê°„ ë‚®ì¶¤
    )
    return _clean_line(resp.output_text)


def generate_dialogue_lines(char_a: dict, char_b: dict, bg_info: dict) -> tuple[str, str]:
    """
    ê°™ì€ ë°°ê²½/ì¸í„°ë™ì…˜ì—ì„œ char_aê°€ ë¨¼ì € í•œ ë§ˆë””,
    char_bê°€ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ í•œ ë§ˆë””.
    â†’ ë‘˜ ë‹¤ ì§§ê³  êµ¬ì–´ì²´.
    Avoid any narration or book-style phrases. The line must sound like spontaneous spoken Korean, not a written script.
    Add small hesitations (ì˜ˆ: 'ì•„...', 'ìŒ...') when appropriate, only if it fits the character.

    """
    place = bg_info.get("background", "")
    action = bg_info.get("interaction", "")
    profile = get_interaction_profile(bg_info)
    emotion_list = "\n".join([f"  - {e}" for e in profile['emotion_options']])

    # Aì˜ ì²« ë§ˆë””
    char_a_data = CHARACTERS.get(char_a['book_code'], {}).get(char_a['role_key'], {})
    char_a_speech = char_a_data.get('speech_patterns', {})
    char_a_style = char_a_speech.get('speaking_style', '')
    
    system_a = (
        "ë‹¹ì‹ ì€ í•œêµ­ ì˜›ì´ì•¼ê¸° ì† ë“±ì¥ì¸ë¬¼ì´ ì‹¤ì œë¡œ ë§í•˜ëŠ” ëŒ€ì‚¬ë¥¼ ì“°ëŠ” ì‘ê°€ì…ë‹ˆë‹¤. "
        "ì²« ë²ˆì§¸ ì¸ë¬¼ì´ íˆ­ ë‚´ë±‰ëŠ” ì§§ì€ í•œ ë§ˆë””ë¥¼ ë§Œë“œì„¸ìš”."
    )
    user_a = f"""
ë°°ê²½ ì¥ì†Œ: {place}
ë°°ê²½ ì¸í„°ë™ì…˜: {action}

ì¥ë©´ ë¶„ìœ„ê¸°:
- ìš”ì•½: {profile['summary']}
- ê°€ëŠ¥í•œ ê°ì •ë“¤ (ìºë¦­í„° ì„±ê²©ì— ë§ëŠ” ê²ƒì„ ì„ íƒí•˜ì„¸ìš”):
{emotion_list}

ì²« ë²ˆì§¸ ì¸ë¬¼ ì„¤ì •(ì˜ì–´): {char_a['personality']}
ì²« ë²ˆì§¸ ì¸ë¬¼ ì •ë³´: {char_a['age']}ì‚´ {char_a['gender']}
ì²« ë²ˆì§¸ ì¸ë¬¼ ë§íˆ¬ ìŠ¤íƒ€ì¼: {char_a_style}

ìƒí™©:
- ì²« ë²ˆì§¸ ì¸ë¬¼ì´ '{action}' ì¥ë©´ ì†ì—ì„œ ìœ„ ê°ì • ì¤‘ ìì‹ ì˜ ì„±ê²©ì— ë§ëŠ” ê²ƒì„ ëŠë¼ë©° ì§§ê²Œ ë§í•©ë‹ˆë‹¤.
- ë°°ê²½ ì¥ì†Œì™€ ì¸í„°ë™ì…˜ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê³  ë§¥ë½ì— ë§ëŠ” ëŒ€ì‚¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
- ì˜ˆë¥¼ ë“¤ì–´, í† ë¼ ê°™ì€ ì¥ë‚œê¾¸ëŸ¬ê¸° ìºë¦­í„°ëŠ” 'ê³ ë°±í•  ê¸°íšŒ' ê°™ì€ ì´ìƒí•œ í‘œí˜„ì„ ì“°ì§€ ë§ê³ , 
  í˜„ì¬ ìƒí™©ê³¼ ë°°ê²½ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”.

ë§íˆ¬ ê·œì¹™:
- ë¬¸ì–´ì²´(ì˜ˆ: '~ê²ƒì´ë‹¤', '~í•©ë‹ˆë‹¤') ëŒ€ì‹  ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
- ìºë¦­í„°ì˜ ë§íˆ¬ ìŠ¤íƒ€ì¼ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”. íŠ¹íˆ '~ì´ê¸°ì•¼' ê°™ì€ ë¹„ë¬¸ë²•ì  í‘œí˜„ì„ ì“°ì§€ ë§ê³  '~ì´ì§€' ê°™ì€ ì˜¬ë°”ë¥¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- ë§¥ë½ì— ë§ì§€ ì•ŠëŠ” ì´ìƒí•œ í‘œí˜„(ì˜ˆ: 'ê³ ë°±í•  ê¸°íšŒ', 'ì•„ë²„ì§€í•œí…Œ ê³ ë°±' ë“±)ì„ í”¼í•˜ê³ , 
  í˜„ì¬ ë°°ê²½ê³¼ ì¸í„°ë™ì…˜ì— ìì—°ìŠ¤ëŸ½ê²Œ ì–´ìš¸ë¦¬ëŠ” ëŒ€ì‚¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
- ìµœëŒ€í•œ ì§§ê³  ê°„ë‹¨í•˜ê²Œ, ì¼ìƒ ëŒ€í™”ì²˜ëŸ¼. (ì˜ˆ: '~í• ê¹Œ?', '~í•˜ëŠ” ê±°ì§€', '~ê°™ì€ë°' ë“±)
- í•œ ë¬¸ì¥ë§Œ, 1~2ì´ˆì— ë§í•  ìˆ˜ ìˆëŠ” ê¸¸ì´.
- ë”°ì˜´í‘œëŠ” ì“°ì§€ ë§ˆì„¸ìš”.
"""
    resp_a = client.responses.create(
        model=TEXT_MODEL,
        input=[
            {"role": "system", "content": system_a},
            {"role": "user", "content": user_a}
        ],
        max_output_tokens=50,
        temperature=0.7
    )
    line_a = _clean_line(resp_a.output_text)

    # Bì˜ ì‘ë‹µ
    char_b_data = CHARACTERS.get(char_b['book_code'], {}).get(char_b['role_key'], {})
    char_b_speech = char_b_data.get('speech_patterns', {})
    char_b_style = char_b_speech.get('speaking_style', '')
    
    system_b = (
        "ë‹¹ì‹ ì€ í•œêµ­ ì˜›ì´ì•¼ê¸° ì† ë‘ ì¸ë¬¼ì´ ì‹¤ì œë¡œ ì£¼ê³ ë°›ëŠ” ëŒ€í™”ë¥¼ ì“°ëŠ” ì‘ê°€ì…ë‹ˆë‹¤. "
        "ë‘ ë²ˆì§¸ ì¸ë¬¼ì´ ì²« ë²ˆì§¸ ì¸ë¬¼ì˜ ë§ì— ë°”ë¡œ ë°˜ì‘í•˜ëŠ” ì§§ì€ í•œ ë§ˆë””ë¥¼ ë§Œë“œì„¸ìš”."
    )
    user_b = f"""
ë°°ê²½ ì¥ì†Œ: {place}
ë°°ê²½ ì¸í„°ë™ì…˜: {action}
ì¥ë©´ ë¶„ìœ„ê¸° ìš”ì•½: {profile['summary']}
ê°€ëŠ¥í•œ ê°ì •ë“¤ (ìºë¦­í„° ì„±ê²©ì— ë§ëŠ” ê²ƒì„ ì„ íƒí•˜ì„¸ìš”):
{emotion_list}

ì²« ë²ˆì§¸ ì¸ë¬¼ì˜ ë§:
{line_a}

ë‘ ë²ˆì§¸ ì¸ë¬¼ ì„¤ì •(ì˜ì–´): {char_b['personality']}
ë‘ ë²ˆì§¸ ì¸ë¬¼ ì •ë³´: {char_b['age']}ì‚´ {char_b['gender']}
ë‘ ë²ˆì§¸ ì¸ë¬¼ ë§íˆ¬ ìŠ¤íƒ€ì¼: {char_b_style}

ìƒí™©:
- ë‘ ë²ˆì§¸ ì¸ë¬¼ì´ ìœ„ ë§ì„ ë“£ê³ , ìì‹ ì˜ ì„±ê²©ì— ë§ëŠ” ê°ì •ìœ¼ë¡œ ë°”ë¡œ ì´ì–´ì„œ í•œ ë§ˆë””ë¥¼ í•©ë‹ˆë‹¤.

ë§íˆ¬ ê·œì¹™:
- ì²« ë²ˆì§¸ ì¸ë¬¼ì˜ ë§ì— ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ëŠ” ë°˜ì‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
- ë¬¸ì–´ì²´ ê¸ˆì§€, ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ë§Œ. (ì˜ˆ: '~ì§€?', '~ì–ì•„', '~ë¼ë‹ˆê¹Œ', '~í•´ìš”' ë“±)
- ìºë¦­í„°ì˜ ë§íˆ¬ ìŠ¤íƒ€ì¼ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”. íŠ¹íˆ '~ì´ê¸°ì•¼' ê°™ì€ ë¹„ë¬¸ë²•ì  í‘œí˜„ì„ ì“°ì§€ ë§ê³  '~ì´ì§€' ê°™ì€ ì˜¬ë°”ë¥¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- í•œ ë¬¸ì¥ë§Œ, ì§§ê²Œ.
- ë”°ì˜´í‘œëŠ” ì“°ì§€ ë§ˆì„¸ìš”.
"""
    resp_b = client.responses.create(
        model=TEXT_MODEL,
        input=[
            {"role": "system", "content": system_b},
            {"role": "user", "content": user_b}
        ],
        max_output_tokens=50,
        temperature=0.7
    )
    line_b = _clean_line(resp_b.output_text)

    return line_a, line_b


def generate_surprised_line(character: dict, bg_info: dict) -> str:
    """
    ë°°ê²½ì´ ê°‘ìê¸° ë°”ë€Œì—ˆì„ ë•Œ ë†€ë¼ëŠ” í•œ ë§ˆë””.
    â†’ ê°íƒ„ + ì§§ì€ êµ¬ì–´ì²´.
    """
    place = bg_info.get("background", "")
    action = bg_info.get("interaction", "")
    profile = get_interaction_profile(bg_info)
    emotion_list = "\n".join([f"  - {e}" for e in profile['emotion_options']])

    system = (
        "ë‹¹ì‹ ì€ í•œêµ­ ì˜›ì´ì•¼ê¸° ì† ë“±ì¥ì¸ë¬¼ì´ ê°‘ìê¸° ë‹¤ë¥¸ ì¥ì†Œë¡œ ì´ë™í–ˆì„ ë•Œì˜ ë°˜ì‘ì„ ì“°ëŠ” ì‘ê°€ì…ë‹ˆë‹¤. "
        "ì‹¤ì œ ì‚¬ëŒì´ ë†€ë¼ì„œ íˆ­ ë‚´ë±‰ëŠ” ì§§ì€ í•œêµ­ì–´ í•œ ë§ˆë””ë¥¼ ë§Œë“œì„¸ìš”."
    )

    # ìºë¦­í„°ì˜ speech_patterns ê°€ì ¸ì˜¤ê¸°
    char_data = CHARACTERS.get(character['book_code'], {}).get(character['role_key'], {})
    speech_patterns = char_data.get('speech_patterns', {})
    frequent_expressions = speech_patterns.get('frequent_expressions', [])
    speaking_style = speech_patterns.get('speaking_style', '')
    
    user = f"""
ìƒˆ ë°°ê²½ ì¥ì†Œ: {place}
ìƒˆ ë°°ê²½ ì¸í„°ë™ì…˜: {action}
ì¥ë©´ ë¶„ìœ„ê¸° ìš”ì•½: {profile['summary']}
ê°€ëŠ¥í•œ ê°ì •ë“¤ (ìºë¦­í„° ì„±ê²©ì— ë§ëŠ” ê²ƒì„ ì„ íƒí•˜ì„¸ìš”):
{emotion_list}

ìºë¦­í„° ì„¤ì •(ì˜ì–´): {character['personality']}
ìºë¦­í„° ì •ë³´: {character['age']}ì‚´ {character['gender']}
ìºë¦­í„° ë§íˆ¬ ìŠ¤íƒ€ì¼: {speaking_style}

ìƒí™©:
- ì´ ìºë¦­í„°ëŠ” ë°©ê¸ˆ ì „ê¹Œì§€ ì „í˜€ ë‹¤ë¥¸ ê³³ì— ìˆì—ˆëŠ”ë°,
  ê°‘ìê¸° ì´ ì¥ë©´ìœ¼ë¡œ ìˆœê°„ì´ë™í•˜ë“¯ ì˜®ê²¨ì¡ŒìŠµë‹ˆë‹¤.
- ìœ„ ê°ì • ì¤‘ ìì‹ ì˜ ì„±ê²©ì— ë§ëŠ” ê²ƒì„ ëŠë¼ë©°, ë†€ë¼ê±°ë‚˜ ë‹¹í™©í•˜ê±°ë‚˜ ì‹ ê¸°í•´ì„œ ê°íƒ„ê³¼ í•¨ê»˜ í•œ ë§ˆë””ë¥¼ í•©ë‹ˆë‹¤.

ë§íˆ¬ ê·œì¹™:
- ì´ ìºë¦­í„°ì˜ ì„±ê²©ê³¼ ë§íˆ¬ ìŠ¤íƒ€ì¼ì— ë§ëŠ” êµ¬ì²´ì ì¸ ê°íƒ„ì‚¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
  ì˜ˆ: ê²ë§ì€ ìºë¦­í„°ëŠ” 'ì–´? ì—¬ê¸°ê°€ ì–´ë””ì§€?', 'ë¬´ì„œìš´ ê³³ì´ë„¤...', 'ì´ìƒí•œ ê³³ì— ì™”ì–´' ë“±
      ìš©ê°í•œ ìºë¦­í„°ëŠ” 'ì˜¤? ì´ê³³ì´ ë°”ë¡œ ê·¸ ê³³ì¸ê°€?', 'í , ì—¬ê¸°ì„œ ë­˜ í•´ì•¼ í•˜ì§€?', 'ë­ì§€, ì´ ë¶„ìœ„ê¸°ëŠ”?' ë“±
      ì¥ë‚œê¾¸ëŸ¬ê¸°ëŠ” 'ì–´? ì´ê±° ì¬ë°Œê² ëŠ”ë°!', 'ì˜¤í˜¸, ì—¬ê¸°ì„œ ë­˜ í•  ìˆ˜ ìˆì„ê¹Œ?', 'ì´ëŸ° ê³³ì´ ìˆì—ˆêµ¬ë‚˜!' ë“±
      ì°¨ë¶„í•œ ìºë¦­í„°ëŠ” 'ì–´ë¼, ì—¬ê¸°ê°€ ì–´ë””ì¼ê¹Œ?', 'ì´ìƒí•˜ë„¤, ë¶„ìœ„ê¸°ê°€ ë‹¬ë¼', 'ìŒ... ì´ê³³ì€ ë­”ê°€ íŠ¹ë³„í•´' ë“±
- 'ì–´ë¼, ì´ê²Œ ë¬´ìŠ¨ ì‹ ê¸°í•œ ì¼ì¸ê°€?', 'ì˜¤í˜¸, ì´ê±° ì¬ë°Œë„¤!' ê°™ì€ ì¼ë°˜ì ì¸ ë©˜íŠ¸ëŠ” í”¼í•˜ê³ , 
  í˜„ì¬ ë°°ê²½ ì¥ì†Œì™€ ì¸í„°ë™ì…˜ì„ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ëŠ” ë†€ë¼ì›€ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- ë¬¸ì–´ì²´ ê¸ˆì§€, ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´.
- í•œ ë¬¸ì¥ë§Œ, ì•„ì£¼ ì§§ê²Œ.
- ë”°ì˜´í‘œëŠ” ì“°ì§€ ë§ˆì„¸ìš”.
"""

    resp = client.responses.create(
        model=TEXT_MODEL,
        input=[
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ],
        max_output_tokens=40,
        temperature=0.7
    )
    return _clean_line(resp.output_text)


def generate_sisters_two_lines(sisters: dict, bg_info: dict) -> tuple[str, str]:
    place = bg_info.get("background", "")
    action = bg_info.get("interaction", "")
    profile = get_interaction_profile(bg_info)
    emotion_list = "\n".join([f"  - {e}" for e in profile['emotion_options']])

    system = (
        "ë‹¹ì‹ ì€ í•œêµ­ ì˜›ì´ì•¼ê¸° 'ì¥í™”í™ë ¨ì „'ì˜ ìë§¤ê°€ ì‹¤ì œë¡œ ì£¼ê³ ë°›ëŠ” ëŒ€ì‚¬ë¥¼ ì“°ëŠ” ì‘ê°€ì…ë‹ˆë‹¤. "
        "ì–¸ë‹ˆì™€ ë™ìƒì´ ì„œë¡œì—ê²Œ í•˜ëŠ” ì§§ì€ êµ¬ì–´ì²´ í•œ ë§ˆë””ì”©, ë‘ ë¬¸ì¥ì„ ë§Œë“œì„¸ìš”."
    )

    user = f"""
ë°°ê²½ ì¥ì†Œ: {place}
ë°°ê²½ ì¸í„°ë™ì…˜: {action}

ì¥ë©´ ë¶„ìœ„ê¸°:
- ìš”ì•½: {profile['summary']}
- ê°€ëŠ¥í•œ ê°ì •ë“¤ (ê° ìë§¤ì˜ ì„±ê²©ì— ë§ëŠ” ê²ƒì„ ì„ íƒí•˜ì„¸ìš”):
{emotion_list}

ìë§¤ ì„¤ì •(ì˜ì–´): {sisters['personality']}
ìë§¤ ì •ë³´: {sisters['age']}ì‚´ {sisters['gender']}

ì¶œë ¥ ê·œì¹™:
- ì²« ë²ˆì§¸ ì¤„: ì–¸ë‹ˆê°€ ë™ìƒì—ê²Œ ë§í•©ë‹ˆë‹¤. ë°˜ë“œì‹œ 'í™ë ¨ì•„' í¬í•¨.
- ë‘ ë²ˆì§¸ ì¤„: ë™ìƒì´ ì–¸ë‹ˆì—ê²Œ ë§í•©ë‹ˆë‹¤. ë°˜ë“œì‹œ 'ì–¸ë‹ˆ' í¬í•¨.
- ë‘ ë¬¸ì¥ ëª¨ë‘ ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: '~ê±°ì•¼', '~í•˜ì§€ ë§ˆ', '~í•´ë³¼ê¹Œ' ë“±)
- íŠ¹íˆ '~ì´ê¸°ì•¼' ê°™ì€ ë¹„ë¬¸ë²•ì  í‘œí˜„ì„ ì“°ì§€ ë§ê³  '~ì´ì§€' ê°™ì€ ì˜¬ë°”ë¥¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- ë¬¸ì–´ì²´ ê¸ˆì§€, ì„¤ëª… ê¸ˆì§€.
- ê°ê° í•œ ë¬¸ì¥ì”©ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

    resp = client.responses.create(
        model=TEXT_MODEL,
        input=[
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ],
        max_output_tokens=80,
        temperature=0.7
    )

    lines = [l.strip() for l in resp.output_text.splitlines() if l.strip()]
    if len(lines) >= 2:
        return _clean_line(lines[0]), _clean_line(lines[1])
    elif len(lines) == 1:
        return _clean_line(lines[0]), "ì–¸ë‹ˆ, ë‚˜ë„ ê·¸ëŸ° ê¸°ë¶„ì´ì•¼."
    else:
        return "í™ë ¨ì•„, ë„ˆë¬´ ê±±ì •í•˜ì§€ ë§ˆ.", "ì–¸ë‹ˆ, ê·¸ë˜ë„ ì¢€ ë¬´ì„œì›Œ."


# ============================================
# 5. TTS
# ============================================
def generate_tts(character: dict, text: str, output_path: str):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    speaker_tag = f"{character['book_code'].upper()}-{character['role_key'].upper()}"
    print(f"ğŸ¤ [{speaker_tag}] line: {text}")

    voice_speed = character.get("speed", 1.0)

    response = client.audio.speech.create(
        model=TTS_MODEL,
        voice=character["voice"],
        input=text,
        response_format="wav",
        speed=voice_speed
    )

    audio_bytes = response.read()

    with open(output_path, "wb") as f:
        f.write(audio_bytes)
    print(f"âœ… Saved: {output_path}")

    return output_path



def play_audio(path: str):
    print(f"ğŸ”Š PLAY AUDIO: {path}")
    subprocess.run(["afplay", path])


# ============================================
# 6. ë©”ì¸ ì§„ì…ì : ì›¹ìº ì—ì„œ book_code + ìˆœì„œ ë„˜ê²¨ì¤„ ë•Œ
# ============================================
def handle_book_input(book_code: str, index_in_sequence: int):
    """
    index_in_sequence ê·œì¹™:

    1: ì´ˆê¸° ë°°ê²½ ì„¤ì •
    2: ì´ˆê¸° cha1 ë“±ì¥ + í•œ ì¤„ ëŒ€ì‚¬
    3: ì´ˆê¸° cha2 ë“±ì¥ + cha1/cha2 ëŒ€í™” (ê° í•œ ì¤„)

    ì´í›„ 4ë¶€í„°ëŠ” 3ê°œ ì£¼ê¸°ë¡œ ë°˜ë³µ:
    4,7,10,... : ë°°ê²½ë§Œ êµì²´ â†’ cha1/cha2 ë‘˜ ë‹¤ ë†€ë¼ëŠ” ëŒ€ì‚¬ í•œ ì¤„ì”©
    5,8,11,... : cha1 êµì²´     â†’ ìƒˆ cha1 + ê¸°ì¡´ cha2 ëŒ€í™” (ê° í•œ ì¤„)
                 (ë‹¨, ìƒˆ cha1ì´ ìë§¤ë©´ ì–¸ë‹ˆ/ë™ìƒ ë‘ ì¤„ + cha2 í•œ ì¤„)
    6,9,12,... : cha2 êµì²´     â†’ ê¸°ì¡´ cha1 + ìƒˆ cha2 ëŒ€í™” (ê° í•œ ì¤„)
    """
    global CURRENT_BG_BOOK_CODE, CURRENT_BG_INFO, CURRENT_CHA1_INFO, CURRENT_CHA2_INFO

    print("\n==============================")
    print(f"[handle_book_input] book_code={book_code}, index={index_in_sequence}")

    # -------------------------
    # 1) index 1: ì´ˆê¸° ë°°ê²½
    # -------------------------
    if index_in_sequence == 1:
        bg = get_background(book_code)
        if bg is None:
            print(f"âš  BACKGROUNDSì— ì—†ëŠ” book_code: {book_code}")
            return

        CURRENT_BG_BOOK_CODE = book_code
        CURRENT_BG_INFO = bg
        CURRENT_CHA1_INFO = None
        CURRENT_CHA2_INFO = None

        print(f"[BACKGROUND INIT] {book_code} â†’ {bg.get('background')}")
        play_background_video(book_code)  # ë°°ê²½ ë¹„ë””ì˜¤ ì¬ìƒ (ë¬´í•œ ë£¨í”„, ì˜¤ë””ì˜¤ í¬í•¨)
        return

    # -------------------------
    # 2) index 2: ì´ˆê¸° cha1
    # -------------------------
    if index_in_sequence == 2:
        if book_code not in ROLE_MAP:
            print(f"âš  ROLE_MAPì— ì—†ëŠ” book_code: {book_code}")
            return
        role_key = ROLE_MAP[book_code]["cha1"]
        if role_key is None:
            print(f"âš  {book_code}ì— cha1 ì •ì˜ ì—†ìŒ")
            return

        cha1 = build_character(book_code, role_key)
        CURRENT_CHA1_INFO = cha1

        # ì¥í™”í™ë ¨ì „ì˜ ê²½ìš° ìë§¤ ë‘˜ ë‹¤ ë§í•˜ë„ë¡
        if book_code == "JHHRJ":
            older, younger = build_sisters_pair()
            CURRENT_CHA1_INFO = older
            CURRENT_CHA2_INFO = younger
            
            # ìë§¤ ë‘˜ ë‹¤ ëŒ€ì‚¬ ìƒì„±
            line1, line2 = generate_sisters_two_lines(older, CURRENT_BG_INFO)
            if not line1 or not line2:
                # ëŒ€ì‚¬ ìƒì„± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ëŒ€ì‚¬ ì‚¬ìš©
                line1 = "í™ë ¨ì•„, ì—¬ê¸°ê°€ ì–´ë””ì§€?"
                line2 = "ì–¸ë‹ˆ, ë‚˜ë„ ëª¨ë¥´ê² ì–´."
            
            out1 = f"output/JHHRJ_sister_older_init_cha1.wav"
            out2 = f"output/JHHRJ_sister_younger_init_cha1.wav"
            generate_tts(older, line1, out1)
            generate_tts(younger, line2, out2)
            play_audio(out1)
            play_audio(out2)
        else:
            line = generate_action_line(cha1, CURRENT_BG_INFO)
            if not line:
                line = f"{CURRENT_BG_INFO.get('interaction', '')}, í•œë²ˆ í•´ë³¼ê¹Œ?"

            out_path = f"output/{book_code}_{role_key}_init_cha1.wav"
            generate_tts(cha1, line, out_path)
            play_audio(out_path)
        return

    # -------------------------
    # 3) index 3: ì´ˆê¸° cha2 + ëŒ€í™”
    # -------------------------
    if index_in_sequence == 3:
        if book_code not in ROLE_MAP:
            print(f"âš  ROLE_MAPì— ì—†ëŠ” book_code: {book_code}")
            return
        role_key = ROLE_MAP[book_code]["cha2"]
        if role_key is None:
            print(f"âš  {book_code}ì— cha2 ì •ì˜ ì—†ìŒ")
            return

        cha2 = build_character(book_code, role_key)
        CURRENT_CHA2_INFO = cha2

        if CURRENT_CHA1_INFO is None:
            print("âš  cha1ì´ ì•„ì§ ì„¤ì •ë˜ì§€ ì•Šì•„ cha2ë§Œ í•œ ì¤„ ëŒ€ì‚¬")
            line2 = generate_action_line(cha2, CURRENT_BG_INFO)
            out2 = f"output/{book_code}_{role_key}_init_cha2_only.wav"
            generate_tts(cha2, line2, out2)
            play_audio(out2)
            return

        # ì¥í™”í™ë ¨ì „ì˜ ê²½ìš°: ë°•ì”¨(cha2)ê°€ ë¨¼ì € ë§í•˜ê³ , ì¥í™”(cha1ì˜ ì–¸ë‹ˆ)ê°€ ë§í•˜ê³ , í™ë ¨(cha1ì˜ ë™ìƒ)ì´ ë§í•¨
        if CURRENT_CHA1_INFO['book_code'] == "JHHRJ" and book_code == "PSJ":
            # ë°•ì”¨ê°€ ë¨¼ì € ë§
            line_psj = generate_action_line(cha2, CURRENT_BG_INFO)
            out_psj = f"output/{book_code}_{role_key}_init_dialog1.wav"
            generate_tts(cha2, line_psj, out_psj)
            
            # ì¥í™”ê°€ ë§
            older, younger = build_sisters_pair()
            line_older = generate_action_line(older, CURRENT_BG_INFO)
            out_older = f"output/JHHRJ_sister_older_init_dialog2.wav"
            generate_tts(older, line_older, out_older)
            
            # í™ë ¨ì´ ë§
            line_younger = generate_action_line(younger, CURRENT_BG_INFO)
            out_younger = f"output/JHHRJ_sister_younger_init_dialog3.wav"
            generate_tts(younger, line_younger, out_younger)
            
            play_audio(out_psj)
            play_audio(out_older)
            play_audio(out_younger)
        else:
            # ìƒˆë¡œ ë“±ì¥í•˜ëŠ” cha2ê°€ ë¨¼ì € ë§í•˜ê³ , cha1ì´ ëŒ€ë‹µí•˜ë„ë¡ ìˆœì„œ ë³€ê²½
            line2, line1 = generate_dialogue_lines(cha2, CURRENT_CHA1_INFO, CURRENT_BG_INFO)
            out2 = f"output/{book_code}_{role_key}_init_dialog1.wav"
            out1 = f"output/{CURRENT_CHA1_INFO['book_code']}_{CURRENT_CHA1_INFO['role_key']}_init_dialog2.wav"
            generate_tts(cha2, line2, out2)
            generate_tts(CURRENT_CHA1_INFO, line1, out1)
            play_audio(out2)
            play_audio(out1)
        return

    # -------------------------
    # 4) ì´í›„: 3ê°œ ì£¼ê¸° (ë°°ê²½ / cha1 / cha2 êµì²´)
    # -------------------------
    if CURRENT_BG_INFO is None or CURRENT_CHA1_INFO is None or CURRENT_CHA2_INFO is None:
        print("âš  ì•„ì§ ì´ˆê¸° 1~3ë²ˆ ì…‹ì—…ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    offset = (index_in_sequence - 4) % 3  # 0,1,2 ë°˜ë³µ

    # ---- 4,7,10,... : ë°°ê²½ êµì²´ + ë‘ ìºë¦­í„° ë†€ëŒ ----
    if offset == 0:
        bg = get_background(book_code)
        if bg is None:
            print(f"âš  BACKGROUNDSì— ì—†ëŠ” book_code: {book_code}")
            return

        CURRENT_BG_BOOK_CODE = book_code
        CURRENT_BG_INFO = bg

        print(f"[BACKGROUND SWAP] {book_code} â†’ {bg.get('background')}")
        play_background_video(book_code)  # ë°°ê²½ ë¹„ë””ì˜¤ êµì²´ (ë¬´í•œ ë£¨í”„, ì˜¤ë””ì˜¤ í¬í•¨, í˜ì´ë“œ íš¨ê³¼)

        line1 = generate_surprised_line(CURRENT_CHA1_INFO, CURRENT_BG_INFO)
        line2 = generate_surprised_line(CURRENT_CHA2_INFO, CURRENT_BG_INFO)

        out1 = f"output/{CURRENT_CHA1_INFO['book_code']}_{CURRENT_CHA1_INFO['role_key']}_surprised.wav"
        out2 = f"output/{CURRENT_CHA2_INFO['book_code']}_{CURRENT_CHA2_INFO['role_key']}_surprised.wav"
        generate_tts(CURRENT_CHA1_INFO, line1, out1)
        generate_tts(CURRENT_CHA2_INFO, line2, out2)
        play_audio(out1)
        play_audio(out2)
        return

        # ---- 5,8,11,... : cha1 êµì²´ ----
    if offset == 1:
        if book_code not in ROLE_MAP:
            print(f"âš  ROLE_MAPì— ì—†ëŠ” book_code: {book_code}")
            return
        role_key = ROLE_MAP[book_code]["cha1"]
        if role_key is None:
            print(f"âš  {book_code}ì— cha1 ì •ì˜ ì—†ìŒ")
            return

        cha1 = build_character(book_code, role_key)
        CURRENT_CHA1_INFO = cha1

        # ğŸ”¸ ì¥í™”í™ë ¨ ìë§¤ì¸ ê²½ìš°: ì–¸ë‹ˆ + ë™ìƒì´ ê°ê° í•œ ì¤„ì”© ë§í•˜ê³ ,
        #    ê¸°ì¡´ cha2(ì˜ˆ: í† ë¼, ê·€ì‹  ë“±)ê°€ í•œ ì¤„ ë” ëŒ€ë‹µ.
        if book_code == "JHHRJ" and role_key == "sister_older":
            sister_older, sister_younger = build_sisters_pair()

            # ì–¸ë‹ˆ â†’ ë™ìƒ ìˆœì„œë¡œ ì„œë¡œ í•œ ì¤„ì”© ëŒ€ì‚¬ ìƒì„±
            lineA, lineB = generate_dialogue_lines(sister_older, sister_younger, CURRENT_BG_INFO)
            reply = generate_action_line(CURRENT_CHA2_INFO, CURRENT_BG_INFO)

            outA = "output/JHHRJ_sister_older_line.wav"
            outB = "output/JHHRJ_sister_younger_line.wav"
            outC = f"output/{CURRENT_CHA2_INFO['book_code']}_{CURRENT_CHA2_INFO['role_key']}_reply_to_sisters.wav"

            # ì–¸ë‹ˆ/ë™ìƒì´ ì„œë¡œ ë‹¤ë¥¸ voiceë¡œ ê°ê° ë§í•˜ê²Œ í•¨
            generate_tts(sister_older, lineA, outA)
            generate_tts(sister_younger, lineB, outB)
            generate_tts(CURRENT_CHA2_INFO, reply, outC)

            play_audio(outA)
            play_audio(outB)
            play_audio(outC)
            return

        # ğŸ”¹ ê·¸ ì™¸ ì¼ë°˜ ìºë¦­í„°: ìƒˆ cha1 + ê¸°ì¡´ cha2ê°€ í•œ ì¤„ì”© ëŒ€í™”
        line1, line2 = generate_dialogue_lines(cha1, CURRENT_CHA2_INFO, CURRENT_BG_INFO)
        out1 = f"output/{book_code}_{role_key}_swapcha1_dialog1.wav"
        out2 = f"output/{CURRENT_CHA2_INFO['book_code']}_{CURRENT_CHA2_INFO['role_key']}_swapcha1_dialog2.wav"
        generate_tts(cha1, line1, out1)
        generate_tts(CURRENT_CHA2_INFO, line2, out2)
        play_audio(out1)
        play_audio(out2)
        return

    # ---- 6,9,12,... : cha2 êµì²´ ----
    if offset == 2:
        if book_code not in ROLE_MAP:
            print(f"âš  ROLE_MAPì— ì—†ëŠ” book_code: {book_code}")
            return
        role_key = ROLE_MAP[book_code]["cha2"]
        if role_key is None:
            print(f"âš  {book_code}ì— cha2 ì •ì˜ ì—†ìŒ")
            return

        cha2 = build_character(book_code, role_key)
        CURRENT_CHA2_INFO = cha2

        # cha1ì´ ì¥í™”í™ë ¨ì¸ ê²½ìš°: cha2ê°€ ë¨¼ì € ë§í•˜ê³ , olderê°€ ë§í•˜ê³ , youngerê°€ ë§í•¨
        if CURRENT_CHA1_INFO['book_code'] == "JHHRJ":
            older, younger = build_sisters_pair()
            
            # cha2ê°€ ë¨¼ì € ë§
            line_cha2 = generate_action_line(cha2, CURRENT_BG_INFO)
            out_cha2 = f"output/{book_code}_{role_key}_swapcha2_dialog1.wav"
            generate_tts(cha2, line_cha2, out_cha2)
            
            # olderê°€ ë§
            line_older = generate_action_line(older, CURRENT_BG_INFO)
            out_older = f"output/JHHRJ_sister_older_swapcha2_dialog2.wav"
            generate_tts(older, line_older, out_older)
            
            # youngerê°€ ë§
            line_younger = generate_action_line(younger, CURRENT_BG_INFO)
            out_younger = f"output/JHHRJ_sister_younger_swapcha2_dialog3.wav"
            generate_tts(younger, line_younger, out_younger)
            
            play_audio(out_cha2)
            play_audio(out_older)
            play_audio(out_younger)
        else:
            # cha2ê°€ ë¨¼ì € ë§í•˜ê³ , cha1ì´ ëŒ€ë‹µí•˜ë„ë¡ ìˆœì„œ ë³€ê²½
            line2, line1 = generate_dialogue_lines(cha2, CURRENT_CHA1_INFO, CURRENT_BG_INFO)
            out2 = f"output/{book_code}_{role_key}_swapcha2_dialog1.wav"
            out1 = f"output/{CURRENT_CHA1_INFO['book_code']}_{CURRENT_CHA1_INFO['role_key']}_swapcha2_dialog2.wav"
            generate_tts(cha2, line2, out2)
            generate_tts(CURRENT_CHA1_INFO, line1, out1)
            play_audio(out2)
            play_audio(out1)
        return


# ============================================
# 7. ì›¹ìº  ArUco ë§ˆì»¤ ê°ì§€
# ============================================
def run_webcam_detection():
    """
    ì›¹ìº ìœ¼ë¡œ ArUco ë§ˆì»¤ë¥¼ ê°ì§€í•˜ê³ , ê°ì§€ëœ ë§ˆì»¤ì— ë”°ë¼ handle_book_inputì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
    ë°°ê²½ ë¹„ë””ì˜¤ëŠ” ë³„ë„ì˜ ìœˆë„ìš°ì—ì„œ ë¶€ë“œëŸ½ê²Œ ì „í™˜ë˜ë©° ë¬´í•œ ë£¨í”„ë¡œ ì¬ìƒë©ë‹ˆë‹¤.
    TTS ìƒì„±ì€ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ì–´ ë¹„ë””ì˜¤ê°€ ëŠê¸°ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    global CURRENT_BG_BOOK_CODE
    
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    print("ğŸ“· Camera . Press 'q' to quit.")
    print("ğŸ“š Show your book to camera...")
    
    # ë¹„ë””ì˜¤ í”Œë ˆì´ì–´ ì‹œì‘
    VIDEO_PLAYER.start()
    
    detector_params = aruco.DetectorParameters()
    detector = aruco.ArucoDetector(ARUCO_DICTIONARY, detector_params)
    
    sequence_index = 0  # í˜„ì¬ ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤
    last_detected_marker = None  # ë§ˆì§€ë§‰ìœ¼ë¡œ ê°ì§€ëœ ë§ˆì»¤ (ì¤‘ë³µ ë°©ì§€)
    handler_thread = None  # handle_book_input ì‹¤í–‰ ìŠ¤ë ˆë“œ
    is_processing = False  # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ì§€ ì—¬ë¶€
    
    # ë¹„ë””ì˜¤ ìœˆë„ìš° ìƒì„± (ì „ì²´ í™”ë©´)
    cv2.namedWindow("Background Video", cv2.WINDOW_NORMAL)
    cv2.setWindowProperty("Background Video", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)
    
    def run_handler_async(book_code, seq_idx):
        """handle_book_inputì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰"""
        nonlocal is_processing
        is_processing = True
        try:
            handle_book_input(book_code, seq_idx)
        finally:
            is_processing = False
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print("âŒ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            break
        
        # ArUco ë§ˆì»¤ ê°ì§€
        corners, ids, rejected = detector.detectMarkers(frame)
        
        # ê°ì§€ëœ ë§ˆì»¤ê°€ ìˆìœ¼ë©´ í‘œì‹œ
        if ids is not None:
            aruco.drawDetectedMarkers(frame, corners, ids)
            
            # ì²« ë²ˆì§¸ë¡œ ê°ì§€ëœ ë§ˆì»¤ ì²˜ë¦¬
            marker_id = ids[0][0]
            book_code = get_book_code_from_marker(marker_id)
            
            # ì´ì „ í•¸ë“¤ëŸ¬ê°€ ì™„ë£Œëœ ê²½ìš°ì—ë§Œ ìƒˆ ë§ˆì»¤ ì²˜ë¦¬
            if book_code and marker_id != last_detected_marker and not is_processing:
                last_detected_marker = marker_id
                sequence_index += 1
                
                # í•œê¸€ ì±… ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                book_info = BACKGROUNDS.get(book_code, {})
                book_name_kr = book_info.get("book", book_code)
                
                print(f"\nğŸ¯ Marker Detected! ID: {marker_id} â†’ {book_name_kr} ({book_code}) (Num of books: {sequence_index})")
                
                # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ handle_book_input ì‹¤í–‰ (ë¹„ë””ì˜¤ê°€ ëŠê¸°ì§€ ì•Šë„ë¡)
                handler_thread = threading.Thread(
                    target=run_handler_async, 
                    args=(book_code, sequence_index),
                    daemon=True
                )
                handler_thread.start()
        
        # í™”ë©´ì— ì •ë³´ í‘œì‹œ (ì›¹ìº  ìœˆë„ìš°)
        cv2.putText(frame, f"Sequence: {sequence_index}", (10, 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        if last_detected_marker is not None:
            book = get_book_code_from_marker(last_detected_marker) or "Unknown"
            cv2.putText(frame, f"Last: {book}", (10, 70), 
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        if is_processing:
            cv2.putText(frame, "Processing...", (10, 110), 
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 165, 255), 2)
        
        cv2.imshow("ArUco Marker Detection", frame)
        
        # ë°°ê²½ ë¹„ë””ì˜¤ í”„ë ˆì„ í‘œì‹œ (ê°™ì€ ìœˆë„ìš°ì—ì„œ ë¶€ë“œëŸ½ê²Œ ì „í™˜)
        video_frame = VIDEO_PLAYER.get_frame()
        if video_frame is not None:
            cv2.imshow("Background Video", video_frame)
        
        # 'q' í‚¤ë¡œ ì¢…ë£Œ
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    # ì •ë¦¬
    cap.release()
    VIDEO_PLAYER.stop()
    cv2.destroyAllWindows()
    print("\nğŸ“· ì›¹ìº  ì¢…ë£Œë¨.")


# ============================================
# 8. ë‹¨ë… ì‹¤í–‰
# ============================================
if __name__ == "__main__":
    import sys
    
    # ArUco ë§ˆì»¤ ìƒì„± ì˜µì…˜
    if len(sys.argv) > 1 and sys.argv[1] == "--generate-markers":
        generate_aruco_markers()
        sys.exit(0)
    
    # ê¸°ë³¸: ì›¹ìº  ê°ì§€ ëª¨ë“œ ì‹¤í–‰
    run_webcam_detection()