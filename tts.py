import os
import json
import subprocess
import threading
import time
import random
from openai import OpenAI
from dotenv import load_dotenv
import cv2
import cv2.aruco as aruco

# ============================================
# 0. ê³µí†µ ì„¤ì •
# ============================================
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

TTS_MODEL = "gpt-4o-mini-tts"   # ìŒì„± ìƒì„± ëª¨ë¸
TEXT_MODEL = "gpt-4o-mini"      # ëŒ€ì‚¬ ìƒì„± ëª¨ë¸

ALLOWED_VOICES = {
    "alloy", "echo", "fable", "onyx", "nova", "shimmer",
    "coral", "verse", "ballad", "ash", "sage", "marin", "cedar"
}

# í˜„ì¬ ìƒíƒœ (ë°°ê²½ / ìºë¦­í„°)
CURRENT_BG_BOOK_CODE = None      # í˜„ì¬ ë°°ê²½ì´ ëœ ì±… ì½”ë“œ
CURRENT_BG_INFO = None           # í˜„ì¬ ë°°ê²½ ì •ë³´
CURRENT_CHA1_INFO = None         # í˜„ì¬ cha1 ìºë¦­í„° dict
CURRENT_CHA2_INFO = None         # í˜„ì¬ cha2 ìºë¦­í„° dict

# ë¹„ë””ì˜¤ í”Œë ˆì´ì–´ (ìŠ¤ë ˆë“œ ê¸°ë°˜)
class VideoPlayer:
    """OpenCV ê¸°ë°˜ ë¹„ë””ì˜¤ í”Œë ˆì´ì–´ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ë¬´í•œ ë£¨í”„ ì¬ìƒ, ì˜¤ë””ì˜¤ í¬í•¨)"""
    
    def __init__(self):
        self.current_video_path = None
        self.video_cap = None
        self.next_video_path = None
        self.frame = None
        self.lock = threading.Lock()
        self.running = False
        self.thread = None
        self.audio_process = None  # ì˜¤ë””ì˜¤ ì¬ìƒ í”„ë¡œì„¸ìŠ¤
        self.bgm_process = None  # BGM ì¬ìƒ í”„ë¡œì„¸ìŠ¤ (ìŠ¤ë ˆë“œ ë˜ëŠ” í”„ë¡œì„¸ìŠ¤)
        self.bgm_proc_ref = None  # BGM í”„ë¡œì„¸ìŠ¤ ì°¸ì¡° (ì‹¤ì œ ì¢…ë£Œìš©)
        self.pending_bgm_path = None  # ì œëª© ë§í•˜ê¸° í›„ ì¬ìƒí•  BGM ê²½ë¡œ
        self.fade_alpha = 1.0  # í˜ì´ë“œ ì•ŒíŒŒ ê°’ (0.0 ~ 1.0)
        self.is_fading = False  # í˜ì´ë“œ ì¤‘ì¸ì§€ ì—¬ë¶€
        self.fade_duration = 0.5  # í˜ì´ë“œ ì§€ì† ì‹œê°„ (ì´ˆ)
        self.fade_start_time = None
        self.overlay_video_cap = None  # ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ch1 (ìºë¦­í„° ì›€ì§ì„)
        self.overlay_video_path = None  # ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ch1 ê²½ë¡œ
        self.overlay_video_cap2 = None  # ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ch2 (ìºë¦­í„° ì›€ì§ì„)
        self.overlay_video_path2 = None  # ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ch2 ê²½ë¡œ
        self.bg_fps = 30.0  # ë°°ê²½ ë¹„ë””ì˜¤ FPS (ê¸°ë³¸ê°’)
        self.overlay_fps = 30.0  # ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ch1 FPS (ê¸°ë³¸ê°’)
        self.overlay_fps2 = 30.0  # ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ch2 FPS (ê¸°ë³¸ê°’)
        self.last_frame_time = None  # ë§ˆì§€ë§‰ í”„ë ˆì„ í‘œì‹œ ì‹œê°„
    
    def _play_loop(self):
        """ë¹„ë””ì˜¤ ì¬ìƒ ë£¨í”„ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
        while self.running:
            loop_start_time = time.time()
            with self.lock:
                # í˜ì´ë“œ íš¨ê³¼ ì²˜ë¦¬
                if self.is_fading and self.fade_start_time:
                    elapsed = time.time() - self.fade_start_time
                    if elapsed < self.fade_duration:
                        # í˜ì´ë“œì•„ì›ƒ: 1.0 -> 0.0
                        self.fade_alpha = 1.0 - (elapsed / self.fade_duration)
                    elif elapsed < self.fade_duration * 2:
                        # ë¹„ë””ì˜¤ ì „í™˜
                        if self.next_video_path and self.next_video_path != self.current_video_path:
                            self._switch_video_internal(self.next_video_path)
                            self.next_video_path = None
                        # í˜ì´ë“œì¸: 0.0 -> 1.0
                        self.fade_alpha = (elapsed - self.fade_duration) / self.fade_duration
                    else:
                        # í˜ì´ë“œ ì™„ë£Œ
                        self.is_fading = False
                        self.fade_alpha = 1.0
                        self.fade_start_time = None
                
                if self.video_cap is None or not self.video_cap.isOpened():
                    time.sleep(0.01)
                    continue
                
                ret, frame = self.video_cap.read()
                if not ret:
                    # ë¹„ë””ì˜¤ ëë‚˜ë©´ ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°€ê¸° (ë¬´í•œ ë£¨í”„)
                    self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                    ret, frame = self.video_cap.read()
                
                if ret:
                    # í˜ì´ë“œ íš¨ê³¼ ì ìš©
                    if self.is_fading and self.fade_alpha < 1.0:
                        # ê²€ì€ í™”ë©´ê³¼ ë¸”ë Œë”©
                        black_frame = frame.copy()
                        black_frame.fill(0)
                        frame = cv2.addWeighted(frame, self.fade_alpha, black_frame, 1.0 - self.fade_alpha, 0)
                    
                    # ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ch1ì´ ìˆìœ¼ë©´ ë°°ê²½ ìœ„ì— í•©ì„±
                    if self.overlay_video_cap is not None and self.overlay_video_cap.isOpened():
                        overlay_ret, overlay_frame = self.overlay_video_cap.read()
                        if not overlay_ret:
                            # ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ëë‚˜ë©´ ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°€ê¸°
                            self.overlay_video_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                            overlay_ret, overlay_frame = self.overlay_video_cap.read()
                        
                        if overlay_ret:
                            # ì˜¤ë²„ë ˆì´ í”„ë ˆì„ í¬ê¸°ë¥¼ ë°°ê²½ í”„ë ˆì„ í¬ê¸°ì— ë§ì¶¤
                            if overlay_frame.shape[:2] != frame.shape[:2]:
                                overlay_frame = cv2.resize(overlay_frame, (frame.shape[1], frame.shape[0]))
                            
                            # ì•ŒíŒŒ ì±„ë„ì´ ìˆìœ¼ë©´ ì•ŒíŒŒ ë¸”ë Œë”©, ì—†ìœ¼ë©´ ì¼ë°˜ ì˜¤ë²„ë ˆì´
                            if overlay_frame.shape[2] == 4:
                                # RGBA -> RGB ë³€í™˜ ë° ì•ŒíŒŒ ë¸”ë Œë”©
                                overlay_rgb = overlay_frame[:, :, :3]
                                overlay_alpha = overlay_frame[:, :, 3:4] / 255.0
                                frame = (frame * (1 - overlay_alpha) + overlay_rgb * overlay_alpha).astype(frame.dtype)
                            else:
                                # ì•ŒíŒŒ ì±„ë„ì´ ì—†ìœ¼ë©´ ì¼ë°˜ ì˜¤ë²„ë ˆì´ (íˆ¬ëª…ë„ ê°€ì •)
                                # ë°°ê²½ ìœ„ì— ì˜¤ë²„ë ˆì´ í•©ì„±
                                mask = cv2.cvtColor(overlay_frame, cv2.COLOR_BGR2GRAY)
                                mask = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)[1]
                                mask = mask.astype(float) / 255.0
                                mask = cv2.merge([mask, mask, mask])
                                frame = (frame * (1 - mask) + overlay_frame * mask).astype(frame.dtype)
                    
                    # ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ch2ê°€ ìˆìœ¼ë©´ ë°°ê²½ ìœ„ì— í•©ì„± (ch1 ìœ„ì—)
                    if self.overlay_video_cap2 is not None and self.overlay_video_cap2.isOpened():
                        overlay_ret2, overlay_frame2 = self.overlay_video_cap2.read()
                        if not overlay_ret2:
                            # ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ëë‚˜ë©´ ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°€ê¸°
                            self.overlay_video_cap2.set(cv2.CAP_PROP_POS_FRAMES, 0)
                            overlay_ret2, overlay_frame2 = self.overlay_video_cap2.read()
                        
                        if overlay_ret2:
                            # ì˜¤ë²„ë ˆì´ í”„ë ˆì„ í¬ê¸°ë¥¼ ë°°ê²½ í”„ë ˆì„ í¬ê¸°ì— ë§ì¶¤
                            if overlay_frame2.shape[:2] != frame.shape[:2]:
                                overlay_frame2 = cv2.resize(overlay_frame2, (frame.shape[1], frame.shape[0]))
                            
                            # ì•ŒíŒŒ ì±„ë„ì´ ìˆìœ¼ë©´ ì•ŒíŒŒ ë¸”ë Œë”©, ì—†ìœ¼ë©´ ì¼ë°˜ ì˜¤ë²„ë ˆì´
                            if overlay_frame2.shape[2] == 4:
                                # RGBA -> RGB ë³€í™˜ ë° ì•ŒíŒŒ ë¸”ë Œë”©
                                overlay_rgb2 = overlay_frame2[:, :, :3]
                                overlay_alpha2 = overlay_frame2[:, :, 3:4] / 255.0
                                frame = (frame * (1 - overlay_alpha2) + overlay_rgb2 * overlay_alpha2).astype(frame.dtype)
                            else:
                                # ì•ŒíŒŒ ì±„ë„ì´ ì—†ìœ¼ë©´ ì¼ë°˜ ì˜¤ë²„ë ˆì´ (íˆ¬ëª…ë„ ê°€ì •)
                                # ë°°ê²½ ìœ„ì— ì˜¤ë²„ë ˆì´ í•©ì„±
                                mask2 = cv2.cvtColor(overlay_frame2, cv2.COLOR_BGR2GRAY)
                                mask2 = cv2.threshold(mask2, 1, 255, cv2.THRESH_BINARY)[1]
                                mask2 = mask2.astype(float) / 255.0
                                mask2 = cv2.merge([mask2, mask2, mask2])
                                frame = (frame * (1 - mask2) + overlay_frame2 * mask2).astype(frame.dtype)
                    
                    self.frame = frame
            
            # ì‹¤ì œ ë¹„ë””ì˜¤ FPSì— ë§ì¶° í”„ë ˆì„ ê°„ê²© ì¡°ì •
            # ë°°ê²½ ë¹„ë””ì˜¤ì˜ FPSë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš© (ì˜¤ë²„ë ˆì´ê°€ ìˆìœ¼ë©´ ë” ë†’ì€ FPS ì‚¬ìš©)
            target_fps = max(self.bg_fps, 
                           self.overlay_fps if self.overlay_video_cap else 0,
                           self.overlay_fps2 if self.overlay_video_cap2 else 0)
            if target_fps <= 0:
                target_fps = 30.0  # ê¸°ë³¸ê°’
            
            frame_interval = 1.0 / target_fps
            
            # í”„ë ˆì„ ì²˜ë¦¬ ì‹œê°„ ê³ ë ¤í•˜ì—¬ ì •í™•í•œ íƒ€ì´ë°ìœ¼ë¡œ ì¬ìƒ
            elapsed = time.time() - loop_start_time
            sleep_time = max(0, frame_interval - elapsed)
            if sleep_time > 0:
                time.sleep(sleep_time)
    
    def _switch_video_internal(self, video_path: str):
        """ë‚´ë¶€ ë¹„ë””ì˜¤ ì „í™˜ (í˜ì´ë“œ ì¤‘ì— í˜¸ì¶œ)"""
        # ë°°ê²½ì´ ë°”ë€” ë•Œ ì´ì „ BGM ì¦‰ì‹œ ì¢…ë£Œ
        self._stop_bgm_immediately()
        
        # ê¸°ì¡´ ë¹„ë””ì˜¤ í•´ì œ
        if self.video_cap:
            self.video_cap.release()
        
        # ìƒˆ ë¹„ë””ì˜¤ ì—´ê¸°
        self.current_video_path = video_path
        self.video_cap = cv2.VideoCapture(video_path)
        if not self.video_cap.isOpened():
            print(f"âŒ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŒ: {video_path}")
            self.video_cap = None
            self.bg_fps = 30.0  # ê¸°ë³¸ê°’
        else:
            # ì‹¤ì œ ë¹„ë””ì˜¤ FPS ì½ê¸°
            fps = self.video_cap.get(cv2.CAP_PROP_FPS)
            if fps > 0:
                self.bg_fps = fps
            else:
                self.bg_fps = 30.0  # ê¸°ë³¸ê°’
            print(f"ğŸ¬ ë¹„ë””ì˜¤ ì „í™˜: {video_path} (FPS: {self.bg_fps:.2f})")
            
            # ë¹„ë””ì˜¤ íŒŒì¼ëª…ì—ì„œ ì±… ì½”ë“œ ì¶”ì¶œí•˜ì—¬ BGM ê²½ë¡œ ì €ì¥ (ì œëª© ë§í•˜ê¸° í›„ ì¬ìƒ)
            video_filename = os.path.basename(video_path)
            book_code = None
            for code, vfile in BOOK_TO_VIDEO.items():
                if vfile == video_filename:
                    book_code = code
                    break
            
            if book_code:
                bgm_file = BOOK_TO_BGM.get(book_code)
                if bgm_file:
                    bgm_path = os.path.join(BGM_DIR, bgm_file)
                    if os.path.exists(bgm_path):
                        # BGM ê²½ë¡œë¥¼ ì €ì¥ (ì œëª© ë§í•˜ê¸° í›„ ì¬ìƒ)
                        self.pending_bgm_path = bgm_path
                    else:
                        print(f"âš ï¸ BGM íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {bgm_path}")
                else:
                    print(f"âš ï¸ '{book_code}'ì— í•´ë‹¹í•˜ëŠ” BGMì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print(f"âš ï¸ ë¹„ë””ì˜¤ íŒŒì¼ëª…ì—ì„œ ì±… ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {video_filename}")
    
    def _stop_bgm_immediately(self):
        """ê¸°ì¡´ BGMì„ ì¦‰ì‹œ ì¢…ë£Œ"""
        old_bgm_process = self.bgm_process
        old_bgm_proc_ref = self.bgm_proc_ref
        if old_bgm_process or old_bgm_proc_ref:
            try:
                if old_bgm_proc_ref:
                    if isinstance(old_bgm_proc_ref, dict):
                        # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (macOS bgm_control)
                        old_bgm_proc_ref["running"] = False
                        if old_bgm_proc_ref.get("current_proc"):
                            try:
                                old_bgm_proc_ref["current_proc"].terminate()
                                old_bgm_proc_ref["current_proc"].wait(timeout=0.1)
                            except:
                                try:
                                    old_bgm_proc_ref["current_proc"].kill()
                                except:
                                    pass
                    elif isinstance(old_bgm_proc_ref, list):
                        # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° [proc, afplay_proc]
                        for p in old_bgm_proc_ref:
                            if p and hasattr(p, "terminate"):
                                try:
                                    p.terminate()
                                    p.wait(timeout=0.1)
                                except:
                                    try:
                                        p.kill()
                                    except:
                                        pass
                    elif hasattr(old_bgm_proc_ref, "terminate"):
                        try:
                            old_bgm_proc_ref.terminate()
                            old_bgm_proc_ref.wait(timeout=0.1)
                        except:
                            try:
                                old_bgm_proc_ref.kill()
                            except:
                                pass
            except Exception as e:
                print(f"âš ï¸ BGM ì¢…ë£Œ ì˜¤ë¥˜: {e}")
            
            self.bgm_process = None
            self.bgm_proc_ref = None
    
    def _start_bgm(self, bgm_path: str):
        """BGMì„ ë¬´í•œ ë£¨í”„ë¡œ ì¬ìƒ (í˜ì´ë“œì¸ íš¨ê³¼ í¬í•¨)"""
        # ê¸°ì¡´ BGMì„ ì¦‰ì‹œ ì¢…ë£Œ
        self._stop_bgm_immediately()
        
        # BGMì„ ë¬´í•œ ë£¨í”„ë¡œ ì¬ìƒ (í˜ì´ë“œì¸ íš¨ê³¼ í¬í•¨)
        import platform
        is_macos = platform.system() == "Darwin"
        
        try:
            if is_macos:
                # macOS: afplayë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ë¬´í•œ ë£¨í”„ ì¬ìƒ (ë” ì•ˆì •ì )
                fade_duration = 0.5
                
                # ì¢…ë£Œ í”Œë˜ê·¸ë¥¼ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
                bgm_control = {"running": True, "current_proc": None}
                
                def play_bgm_with_fade():
                    try:
                        # ì„ì‹œ íŒŒì¼ì— í˜ì´ë“œì¸ íš¨ê³¼ë¥¼ ì ìš©í•œ BGM ìƒì„± (ì²« ë£¨í”„ë§Œ)
                        import tempfile
                        temp_dir = tempfile.gettempdir()
                        temp_bgm = os.path.join(temp_dir, f"bgm_fade_{os.getpid()}.wav")
                        
                        # ì²« ë£¨í”„ì—ë§Œ í˜ì´ë“œì¸ ì ìš© + ìŒëŸ‰ 50%
                        subprocess.run(
                            ["ffmpeg", "-y", "-i", bgm_path,
                             "-af", f"afade=t=in:st=0:d={fade_duration},volume=0.5",
                             "-acodec", "pcm_s16le", "-ar", "44100", "-ac", "2",
                             temp_bgm],
                            stdout=subprocess.DEVNULL,
                            stderr=subprocess.DEVNULL,
                            check=True
                        )
                        
                        # ì›ë³¸ íŒŒì¼ë„ ìŒëŸ‰ 50%ë¡œ ì¡°ì •í•œ ì„ì‹œ íŒŒì¼ ìƒì„±
                        temp_bgm_loop = os.path.join(temp_dir, f"bgm_loop_{os.getpid()}.wav")
                        subprocess.run(
                            ["ffmpeg", "-y", "-i", bgm_path,
                             "-af", "volume=0.5",
                             "-acodec", "pcm_s16le", "-ar", "44100", "-ac", "2",
                             temp_bgm_loop],
                            stdout=subprocess.DEVNULL,
                            stderr=subprocess.DEVNULL,
                            check=True
                        )
                        
                        # ì²« ë²ˆì§¸ëŠ” í˜ì´ë“œì¸ ì ìš©ëœ íŒŒì¼ ì¬ìƒ
                        first_play = True
                        while bgm_control["running"]:
                            if first_play:
                                proc = subprocess.Popen(
                                    ["afplay", temp_bgm],
                                    stdout=subprocess.DEVNULL,
                                    stderr=subprocess.DEVNULL
                                )
                                bgm_control["current_proc"] = proc
                                proc.wait()
                                first_play = False
                            else:
                                # ì´í›„ëŠ” ìŒëŸ‰ ì¡°ì •ëœ íŒŒì¼ ë¬´í•œ ë£¨í”„
                                proc = subprocess.Popen(
                                    ["afplay", temp_bgm_loop],
                                    stdout=subprocess.DEVNULL,
                                    stderr=subprocess.DEVNULL
                                )
                                bgm_control["current_proc"] = proc
                                proc.wait()
                                
                                # ì¢…ë£Œ ì‹ í˜¸ í™•ì¸
                                if not bgm_control["running"]:
                                    break
                        
                        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                        try:
                            os.remove(temp_bgm)
                            os.remove(temp_bgm_loop)
                        except:
                            pass
                    except Exception as e:
                        print(f"âš ï¸ BGM ì¬ìƒ ì˜¤ë¥˜: {e}")
                        import traceback
                        traceback.print_exc()
                
                # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ BGM ì¬ìƒ
                bgm_thread = threading.Thread(target=play_bgm_with_fade, daemon=True)
                bgm_thread.start()
                self.bgm_process = bgm_thread
                # í”„ë¡œì„¸ìŠ¤ ì°¸ì¡°ëŠ” ì œì–´ ë”•ì…”ë„ˆë¦¬ë¡œ ê´€ë¦¬
                with self.lock:
                    self.bgm_proc_ref = bgm_control
                print(f"ğŸµ BGM ì¬ìƒ ì‹œì‘ (í˜ì´ë“œì¸): {bgm_path}")
            else:
                # Linux: ffplay ì‚¬ìš©
                proc = subprocess.Popen(
                    ["ffmpeg", "-stream_loop", "-1", "-i", bgm_path,
                     "-af", "afade=t=in:st=0:d=0.5,volume=0.5",
                     "-f", "wav", "-"],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.DEVNULL
                )
                ffplay_proc = subprocess.Popen(
                    ["ffplay", "-nodisp", "-autoexit", "-loop", "0", "-loglevel", "quiet", "-"],
                    stdin=proc.stdout,
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL
                )
                self.bgm_process = ffplay_proc
                self.bgm_proc_ref = [proc, ffplay_proc]
                print(f"ğŸµ BGM ì¬ìƒ ì‹œì‘ (í˜ì´ë“œì¸): {bgm_path}")
        except FileNotFoundError:
            print("âš ï¸ ffmpeg/afplayë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. BGMì€ ì¬ìƒë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            if is_macos:
                print("   macOSì—ì„œëŠ” 'brew install ffmpeg'ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
            else:
                print("   Linuxì—ì„œëŠ” 'sudo apt-get install ffmpeg' ë˜ëŠ” 'sudo yum install ffmpeg'ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
    
    def _start_audio(self, video_path: str):
        """ë¹„ë””ì˜¤ì˜ ì˜¤ë””ì˜¤ë¥¼ ë¬´í•œ ë£¨í”„ë¡œ ì¬ìƒ"""
        # ê¸°ì¡´ ì˜¤ë””ì˜¤ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
        if self.audio_process:
            # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (macOS ìŠ¤ë ˆë“œ)
            if isinstance(self.audio_process, dict):
                self.audio_process["ref"]["running"] = False
            # í”„ë¡œì„¸ìŠ¤ì¸ ê²½ìš°
            elif hasattr(self.audio_process, "terminate"):
                try:
                    self.audio_process.terminate()
                    self.audio_process.wait(timeout=1.0)
                except:
                    try:
                        self.audio_process.kill()
                    except:
                        pass
            self.audio_process = None
        
        # ffmpegë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ì˜ ì˜¤ë””ì˜¤ë¥¼ ë¬´í•œ ë£¨í”„ë¡œ ì¬ìƒ
        import platform
        is_macos = platform.system() == "Darwin"
        
        try:
            if is_macos:
                # macOS: ffmpegë¡œ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•˜ê³  afplayë¡œ ì¬ìƒ (ë¬´í•œ ë£¨í”„)
                # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ë¬´í•œ ë£¨í”„ë¡œ ì¬ìƒ
                audio_thread_ref = {"running": True}
                
                def play_audio_loop():
                    while audio_thread_ref["running"]:
                        try:
                            # ffmpegë¡œ ì˜¤ë””ì˜¤ë¥¼ ë¬´í•œ ë£¨í”„ë¡œ ì¬ìƒ (stream_loop ì‚¬ìš©)
                            proc = subprocess.Popen(
                                ["ffmpeg", "-re", "-stream_loop", "-1", "-i", video_path, 
                                 "-vn", "-acodec", "pcm_s16le", "-ar", "44100", "-ac", "2", "-f", "wav", "-"],
                                stdout=subprocess.PIPE,
                                stderr=subprocess.DEVNULL,
                                bufsize=0
                            )
                            # afplayë¡œ ì¬ìƒ (stdinì—ì„œ ì½ê¸°)
                            afplay_proc = subprocess.Popen(
                                ["afplay", "-"],
                                stdin=proc.stdout,
                                stdout=subprocess.DEVNULL,
                                stderr=subprocess.DEVNULL
                            )
                            afplay_proc.wait()
                            proc.wait()
                        except Exception as e:
                            print(f"âš ï¸ ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜: {e}")
                            break
                
                # ì˜¤ë””ì˜¤ ì¬ìƒ ìŠ¤ë ˆë“œ ì‹œì‘
                audio_thread = threading.Thread(target=play_audio_loop, daemon=True)
                audio_thread.start()
                self.audio_process = {"thread": audio_thread, "ref": audio_thread_ref}
            else:
                # Linux: ffplay ì‚¬ìš©
                self.audio_process = subprocess.Popen(
                    ["ffplay", "-nodisp", "-autoexit", "-loop", "0", "-loglevel", "quiet", video_path],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL
                )
        except FileNotFoundError:
            print("âš ï¸ ffmpeg/ffplayë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë””ì˜¤ëŠ” ì¬ìƒë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            if is_macos:
                print("   macOSì—ì„œëŠ” 'brew install ffmpeg'ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
            else:
                print("   Linuxì—ì„œëŠ” 'sudo apt-get install ffmpeg' ë˜ëŠ” 'sudo yum install ffmpeg'ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
    
    def start(self):
        """í”Œë ˆì´ì–´ ì‹œì‘"""
        if not self.running:
            self.running = True
            self.thread = threading.Thread(target=self._play_loop, daemon=True)
            self.thread.start()
    
    def set_overlay_video(self, overlay_path: str):
        """ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ch1 ì„¤ì • (ë°°ê²½ ìœ„ì— í‘œì‹œë  ìºë¦­í„° ì›€ì§ì„)"""
        with self.lock:
            # ê¸°ì¡´ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ í•´ì œ
            if self.overlay_video_cap:
                self.overlay_video_cap.release()
            
            if overlay_path and os.path.exists(overlay_path):
                self.overlay_video_path = overlay_path
                self.overlay_video_cap = cv2.VideoCapture(overlay_path)
                if not self.overlay_video_cap.isOpened():
                    print(f"âŒ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŒ: {overlay_path}")
                    self.overlay_video_cap = None
                    self.overlay_video_path = None
                    self.overlay_fps = 30.0  # ê¸°ë³¸ê°’
                else:
                    # ì‹¤ì œ ë¹„ë””ì˜¤ FPS ì½ê¸°
                    fps = self.overlay_video_cap.get(cv2.CAP_PROP_FPS)
                    if fps > 0:
                        self.overlay_fps = fps
                    else:
                        self.overlay_fps = 30.0  # ê¸°ë³¸ê°’
                    print(f"ğŸ¬ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ch1 ì„¤ì •: {overlay_path} (FPS: {self.overlay_fps:.2f})")
            else:
                self.overlay_video_cap = None
                self.overlay_video_path = None
    
    def set_overlay_video2(self, overlay_path: str):
        """ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ch2 ì„¤ì • (ë°°ê²½ ìœ„ì— í‘œì‹œë  ìºë¦­í„° ì›€ì§ì„)"""
        with self.lock:
            # ê¸°ì¡´ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ch2 í•´ì œ
            if self.overlay_video_cap2:
                self.overlay_video_cap2.release()
            
            if overlay_path and os.path.exists(overlay_path):
                self.overlay_video_path2 = overlay_path
                self.overlay_video_cap2 = cv2.VideoCapture(overlay_path)
                if not self.overlay_video_cap2.isOpened():
                    print(f"âŒ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ch2ë¥¼ ì—´ ìˆ˜ ì—†ìŒ: {overlay_path}")
                    self.overlay_video_cap2 = None
                    self.overlay_video_path2 = None
                    self.overlay_fps2 = 30.0  # ê¸°ë³¸ê°’
                else:
                    # ì‹¤ì œ ë¹„ë””ì˜¤ FPS ì½ê¸°
                    fps = self.overlay_video_cap2.get(cv2.CAP_PROP_FPS)
                    if fps > 0:
                        self.overlay_fps2 = fps
                    else:
                        self.overlay_fps2 = 30.0  # ê¸°ë³¸ê°’
                    print(f"ğŸ¬ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ch2 ì„¤ì •: {overlay_path} (FPS: {self.overlay_fps2:.2f})")
            else:
                self.overlay_video_cap2 = None
                self.overlay_video_path2 = None
    
    def clear_overlay_video(self):
        """ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ëª¨ë‘ ì œê±°"""
        with self.lock:
            if self.overlay_video_cap:
                self.overlay_video_cap.release()
                self.overlay_video_cap = None
                self.overlay_video_path = None
            if self.overlay_video_cap2:
                self.overlay_video_cap2.release()
                self.overlay_video_cap2 = None
                self.overlay_video_path2 = None
                print("ğŸ¬ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ëª¨ë‘ ì œê±°")
    
    def stop(self):
        """í”Œë ˆì´ì–´ ì¤‘ì§€"""
        self.running = False
        if self.thread:
            self.thread.join(timeout=1.0)
        with self.lock:
            if self.video_cap:
                self.video_cap.release()
                self.video_cap = None
            if self.overlay_video_cap:
                self.overlay_video_cap.release()
                self.overlay_video_cap = None
            if self.overlay_video_cap2:
                self.overlay_video_cap2.release()
                self.overlay_video_cap2 = None
            self.frame = None
        # ì˜¤ë””ì˜¤ í”„ë¡œì„¸ìŠ¤/ìŠ¤ë ˆë“œ ì¢…ë£Œ
        if self.audio_process:
            # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (macOS ìŠ¤ë ˆë“œ)
            if isinstance(self.audio_process, dict):
                self.audio_process["ref"]["running"] = False
            # í”„ë¡œì„¸ìŠ¤ì¸ ê²½ìš° (Linux)
            elif hasattr(self.audio_process, "terminate"):
                try:
                    self.audio_process.terminate()
                    self.audio_process.wait(timeout=1.0)
                except:
                    try:
                        self.audio_process.kill()
                    except:
                        pass
            self.audio_process = None
        
        # BGM í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
        if self.bgm_proc_ref:
            if isinstance(self.bgm_proc_ref, dict):
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (macOS bgm_control)
                self.bgm_proc_ref["running"] = False
                if self.bgm_proc_ref.get("current_proc"):
                    try:
                        self.bgm_proc_ref["current_proc"].terminate()
                        self.bgm_proc_ref["current_proc"].wait(timeout=0.5)
                    except:
                        try:
                            self.bgm_proc_ref["current_proc"].kill()
                        except:
                            pass
            elif isinstance(self.bgm_proc_ref, list):
                # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° [proc, afplay_proc]
                for p in self.bgm_proc_ref:
                    if p and hasattr(p, "terminate"):
                        try:
                            p.terminate()
                            p.wait(timeout=0.5)
                        except:
                            try:
                                p.kill()
                            except:
                                pass
            elif hasattr(self.bgm_proc_ref, "terminate"):
                try:
                    self.bgm_proc_ref.terminate()
                    self.bgm_proc_ref.wait(timeout=0.5)
                except:
                    try:
                        self.bgm_proc_ref.kill()
                    except:
                        pass
            self.bgm_proc_ref = None
        self.bgm_process = None
    
    def set_video(self, video_path: str):
        """ë¹„ë””ì˜¤ íŒŒì¼ ë³€ê²½ (í˜ì´ë“œ íš¨ê³¼ì™€ í•¨ê»˜ ë¶€ë“œëŸ¬ìš´ ì „í™˜)"""
        with self.lock:
            if self.current_video_path is None:
                # ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ëŠ” í˜ì´ë“œ ì—†ì´ ë°”ë¡œ ì‹œì‘
                self._switch_video_internal(video_path)
            else:
                # ë‹¤ìŒ ë¹„ë””ì˜¤ë¡œ ì „í™˜ (í˜ì´ë“œ íš¨ê³¼)
                self.next_video_path = video_path
                self.is_fading = True
                self.fade_start_time = time.time()
    
    def get_frame(self):
        """í˜„ì¬ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°"""
        with self.lock:
            if self.frame is not None:
                return self.frame.copy()
        return None

# ì „ì—­ ë¹„ë””ì˜¤ í”Œë ˆì´ì–´ ì¸ìŠ¤í„´ìŠ¤
VIDEO_PLAYER = VideoPlayer()

# ë°°ê²½ ë¹„ë””ì˜¤ ì„¤ì •
BG_VIDEO_DIR = "bg_video"
BGM_DIR = "bgm"
BOOK_TO_VIDEO = {
    "BJBJ": "10_BJBJ_matchedSize.mov",
    "PSJ": "11_PSJ_matchedSize.mov",
    "DGJ": "13_DGJ_matchedSize.mov",
    "HBJ": "17_HBJ_matchedSize.mov",
    "JWCJ": "19_JWCJ_matchedSize.mov",
    "KWJ": "3_KWJ_matchedSize.mov",
    "OGJJ": "5_OGJJ_matchedSize.mov",
    "JHHRJ": "6_JHHRJ_matchedSize.mov",
    "SCJ": "7_SCJ_matchedSize.mov",
}
# ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ íŒŒì¼ëª… ë§¤í•‘ (inter_video í´ë”ì˜ íŒŒì¼ëª… í˜•ì‹)
BOOK_TO_OVERLAY_CODE = {
    "BJBJ": "BJBJ",
    "PSJ": "BSJ",  # ë°•ì”¨ì „ -> BSJ
    "DGJ": "DCJ",  # ë•ìºë¹„ì „ -> DCJ
    "HBJ": "HBJ",
    "JWCJ": "JWCJ",
    "KWJ": "KWJ",
    "OGJJ": "OGJJ",
    "JHHRJ": "JHHRJ",
    "SCJ": "SCJ",
}

BOOK_TO_BGM = {
    "BJBJ": "10_BJBJ_audioExtracted.wav",
    "PSJ": "11_BSJ_audioExtracted.wav",  # íŒŒì¼ëª…ì´ BSJë¡œ ë˜ì–´ ìˆìŒ
    "DGJ": "13_DGJ_audioExtracted.wav",
    "HBJ": "17_HBJ_audioExtracted.wav",
    "JWCJ": "19_JWCJ_audioExtracted.wav",
    "KWJ": "3_KWJ_audioExtracted.wav",
    "OGJJ": "5_OGJJ_audioExtracted.wav",
    "JHHRJ": "6_JHHRJ_audioExtracted.wav",
    "SCJ": "7_SCJ_audioExtracted.wav",
}


# ============================================
# 1. ì„¤ì • íŒŒì¼ ë¡œë“œ
# ============================================
def load_json(path: str):
    if not os.path.exists(path):
        raise FileNotFoundError(f"{path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

CHARACTERS = load_json("characters.json")
BACKGROUNDS = load_json("backgrounds.json")

# ì±… ì½”ë“œ â†’ cha1 / cha2 ì—­í•  í‚¤ ë§¤í•‘
ROLE_MAP = {
    "SCJ": {"cha1": "simcheong",    "cha2": "simbongsa"},
    "HBJ": {"cha1": "heungbu",      "cha2": "nolbu"},
    "BJBJ": {"cha1": "turtle",      "cha2": "rabbit"},
    "OGJJ": {"cha1": "onggojip",    "cha2": "onggojip"},
    "JWCJ": {"cha1": "jeonwoochi",  "cha2": "jeonwoochi"},
    "JHHRJ": {"cha1": "sister_older",    "cha2": "ghost"},
    "PSJ": {"cha1": "ugly",         "cha2": "pretty"},
    "DGJ": {"cha1": "toad",         "cha2": "fox"},
    "KWJ": {"cha1": "kimwon",       "cha2": "monster"}
}

# ============================================
# ArUco ë§ˆì»¤ ì„¤ì •
# ============================================
ARUCO_DICTIONARY = aruco.getPredefinedDictionary(aruco.DICT_5X5_1000)
ARUCO_MARKER_SIZE = 200  # pixels

# ë§ˆì»¤ ID â†’ ì±… ì½”ë“œ ë§¤í•‘
MARKER_TO_BOOK = {
    1: "KWJ",      # 03_KWJ
    2: "PSJ",      # 11_PSJ
    6: "DGJ",      # 13_DGJ
    7: "JHHRJ",    # 06_JHHRJ
    8: "JWCJ",     # 19_JWCJ
    9: "HBJ",      # 17_HBJ
    10: "OGJJ",    # 05_OGJJ
    11: "SCJ",     # 07_SCJ
    12: "BJBJ",    # 10_BJBJ
}

# ë§ˆì»¤ ID â†’ íŒŒì¼ëª… ë§¤í•‘
MARKER_NAMES = {
    1: "03_KWJ",
    2: "11_PSJ",
    6: "13_DGJ",
    7: "06_JHHRJ",
    8: "19_JWCJ",
    9: "17_HBJ",
    10: "05_OGJJ",
    11: "07_SCJ",
    12: "10_BJBJ",
}


def generate_aruco_markers(output_dir: str = "markers"):
    """
    ArUco ë§ˆì»¤ ì´ë¯¸ì§€ë“¤ì„ ìƒì„±í•˜ì—¬ ì§€ì •ëœ í´ë”ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    os.makedirs(output_dir, exist_ok=True)
    
    for marker_id, name in MARKER_NAMES.items():
        marker_image = aruco.generateImageMarker(ARUCO_DICTIONARY, marker_id, ARUCO_MARKER_SIZE)
        filename = os.path.join(output_dir, f"{name}.png")
        cv2.imwrite(filename, marker_image)
        print(f"âœ… Saved ArUco marker: {filename}")
    
    print(f"\nğŸ¯ ì´ {len(MARKER_NAMES)}ê°œì˜ ArUco ë§ˆì»¤ê°€ '{output_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def get_book_code_from_marker(marker_id: int) -> str | None:
    """
    ArUco ë§ˆì»¤ IDë¡œë¶€í„° ì±… ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    return MARKER_TO_BOOK.get(marker_id)

# ============================================
# 2. Background ê´€ë ¨
# ============================================
def get_background(book_code: str):
    return BACKGROUNDS.get(book_code)

# ë°°ê²½ ìŒì•…ì€ ì´ì œ ë¹„ë””ì˜¤ì— í¬í•¨ëœ ì˜¤ë””ì˜¤ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ í•¨ìˆ˜ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
# def play_background_music_if_exists(bg_info: dict):
#     pass


def stop_background_video():
    """í˜„ì¬ ì¬ìƒ ì¤‘ì¸ ë°°ê²½ ë¹„ë””ì˜¤ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤."""
    VIDEO_PLAYER.stop()
    print("ğŸ¬ ë°°ê²½ ë¹„ë””ì˜¤ ì¤‘ì§€ë¨")


def play_background_video(book_code: str):
    """
    ì±… ì½”ë“œì— í•´ë‹¹í•˜ëŠ” ë°°ê²½ ë¹„ë””ì˜¤ë¥¼ ë¬´í•œ ë£¨í”„ë¡œ ì¬ìƒí•©ë‹ˆë‹¤.
    ê°™ì€ ìœˆë„ìš°ì—ì„œ ë¶€ë“œëŸ½ê²Œ ì „í™˜ë©ë‹ˆë‹¤.
    """
    # ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ í™•ì¸
    video_file = BOOK_TO_VIDEO.get(book_code)
    if video_file is None:
        print(f"ğŸ¬ '{book_code}'ì— í•´ë‹¹í•˜ëŠ” ë°°ê²½ ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    video_path = os.path.join(BG_VIDEO_DIR, video_file)
    if not os.path.exists(video_path):
        print(f"ğŸ¬ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {video_path}")
        return
    
    # VideoPlayerë¥¼ í†µí•´ ë¹„ë””ì˜¤ ì „í™˜ (ê°™ì€ ìœˆë„ìš°ì—ì„œ ë¶€ë“œëŸ½ê²Œ)
    VIDEO_PLAYER.set_video(video_path)

def get_interaction_profile(bg_info: dict) -> dict:
    """
    backgrounds.json ì•ˆì— ë¯¸ë¦¬ ì •ì˜í•´ ë‘”
    interaction_label / interaction_summary / interaction_emotionsë¥¼ ê°€ì ¸ì˜¨ë‹¤.
    interaction_emotionsëŠ” 10ê°€ì§€ ê°ì • ì˜µì…˜ ë¦¬ìŠ¤íŠ¸ë¡œ, LLMì´ ìºë¦­í„° ì„±ê²©ì— ë§ê²Œ ì„ íƒí•œë‹¤.
    """
    if bg_info is None:
        return {
            "label": "neutral",
            "summary": "A neutral situation with no special context",
            "emotion_options": ["mild curiosity", "calm observation", "quiet interest"]
        }

    # interaction_emotionsëŠ” ì´ì œ ë¦¬ìŠ¤íŠ¸
    emotions_data = bg_info.get("interaction_emotions", [])
    if isinstance(emotions_data, list):
        emotion_options = emotions_data
    else:
        # í˜¹ì‹œ ë¬¸ìì—´ì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        emotion_options = [emotions_data]

    return {
        "label": bg_info.get("interaction_label", "neutral"),
        "summary": bg_info.get(
            "interaction_summary",
            f"A scene involving '{bg_info.get('interaction', '')}'"
        ),
        "emotion_options": emotion_options
    }

# ============================================
# 3. Character ê´€ë ¨
# ============================================
def build_character(book_code: str, role_key: str) -> dict:
    data = CHARACTERS[book_code][role_key]

    gender = data["gender"]
    age = data["age"]
    base_desc = data["base_personality"]
    raw_voice = data.get("voice", "alloy")
    speed = data.get("speed", 1.0)

    voice = raw_voice if raw_voice in ALLOWED_VOICES else "alloy"

    personality = base_desc

    return {
        "book_code": book_code,
        "role_key": role_key,
        "gender": gender,
        "age": age,
        "voice": voice,
        "personality": personality,
        "speed": speed,
    }

def build_sisters_pair() -> tuple[dict, dict]:
    older = build_character("JHHRJ", "sister_older")
    younger = build_character("JHHRJ", "sister_younger")
    return older, younger

# ============================================
# 4. í…ìŠ¤íŠ¸ LLMìœ¼ë¡œ ëŒ€ì‚¬ ìƒì„±
# ============================================
def _clean_line(text: str) -> str:
    if not text:
        return ""
    line = text.strip().splitlines()[0]
    line = line.strip().strip("ã€Œã€\"'â€œâ€â€˜â€™")
    return line


def generate_action_line(character: dict, bg_info: dict) -> str:
    """
    ë°°ê²½/ì¸í„°ë™ì…˜ì„ ë³´ê³  ìºë¦­í„°ê°€ ê·¸ í–‰ë™ì„ í•˜ê¸° ì§ì „ì— í•˜ëŠ” í•œ ë§ˆë””.
    â†’ ìµœëŒ€í•œ ì§§ê³  êµ¬ì–´ì²´, ì‚¬ëŒ ë§ì²˜ëŸ¼.
    """
    place = bg_info.get("background", "")
    action = bg_info.get("interaction", "")
    profile = get_interaction_profile(bg_info)
    emotion_list = "\n".join([f"  - {e}" for e in profile['emotion_options']])

    # ìºë¦­í„°ì˜ speech_patterns ê°€ì ¸ì˜¤ê¸°
    char_data = CHARACTERS.get(character['book_code'], {}).get(character['role_key'], {})
    speech_patterns = char_data.get('speech_patterns', {})
    speaking_style = speech_patterns.get('speaking_style', '')

    system = (
        "ë‹¹ì‹ ì€ í•œêµ­ ì˜›ì´ì•¼ê¸° ì† ë“±ì¥ì¸ë¬¼ì´ ì‹¤ì œë¡œ ë§í•˜ëŠ” ëŒ€ì‚¬ë¥¼ ì“°ëŠ” ì‘ê°€ì…ë‹ˆë‹¤. "
        "ëŒ€ë³¸ì´ë‚˜ ë‚˜ë ˆì´ì…˜ì´ ì•„ë‹ˆë¼, ì‚¬ëŒì´ ì…ìœ¼ë¡œ íˆ­ íŠ€ì–´ë‚˜ì˜¤ê²Œ ë§í•˜ëŠ” í•œêµ­ì–´ êµ¬ì–´ì²´ë¥¼ ë§Œë“œì„¸ìš”."
    )

    user = f"""
ë°°ê²½ ì¥ì†Œ: {place}
ë°°ê²½ ì¸í„°ë™ì…˜: {action}

ì¥ë©´ ë¶„ìœ„ê¸°:
- ìš”ì•½: {profile['summary']}
- ê°€ëŠ¥í•œ ê°ì •ë“¤ (ìºë¦­í„° ì„±ê²©ì— ë§ëŠ” ê²ƒì„ ì„ íƒí•˜ì„¸ìš”):
{emotion_list}

ìºë¦­í„° ì„¤ì •(ì˜ì–´): {character['personality']}
ìºë¦­í„° ì •ë³´: {character['age']}ì‚´ {character['gender']}
ìºë¦­í„° ë§íˆ¬ ìŠ¤íƒ€ì¼: {speaking_style}

ìƒí™©:
- ì´ ìºë¦­í„°ê°€ ì§€ê¸ˆ '{action}'ì„(ë¥¼) í•˜ê¸° ì§ì „ì…ë‹ˆë‹¤.
- ìœ„ ê°ì • ì˜µì…˜ ì¤‘ ì´ ìºë¦­í„°ì˜ ì„±ê²©ì— ê°€ì¥ ì–´ìš¸ë¦¬ëŠ” ê°ì •ì„ ì„ íƒí•˜ê³ , ê·¸ ê°ì •ì„ ë‹´ì•„ ì§§ê²Œ í•œ ë§ˆë””ë¥¼ í•©ë‹ˆë‹¤.

ë§íˆ¬ ê·œì¹™:
- ë¬¸ì–´ì²´(ì˜ˆ: '~ê²ƒì´ë‹¤', '~í•©ë‹ˆë‹¤')ë¥¼ ì ˆëŒ€ ì“°ì§€ ë§ˆì„¸ìš”.
- ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ë§Œ ì“°ì„¸ìš”. (ì˜ˆ: '~ê±°ì•¼', '~í•˜ëŠ” ê±´ê°€?', '~í•´ë³¼ê¹Œ', '~í•˜ë„¤ìš”' ë“±)
- ìºë¦­í„°ì˜ ë§íˆ¬ ìŠ¤íƒ€ì¼ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”. íŠ¹íˆ '~ì´ê¸°ì•¼' ê°™ì€ ë¹„ë¬¸ë²•ì  í‘œí˜„ì„ ì“°ì§€ ë§ê³  '~ì´ì§€' ê°™ì€ ì˜¬ë°”ë¥¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- ë„ˆë¬´ ê¸¸ê²Œ ì„¤ëª…í•˜ì§€ ë§ê³ , 1~2ì´ˆ ì•ˆì— ë§í•  ìˆ˜ ìˆì„ ì •ë„ì˜ ì§§ì€ í•œ ë¬¸ì¥ìœ¼ë¡œ.
- ëŠë‚Œí‘œë‚˜ ë¬¼ìŒí‘œëŠ” ì¨ë„ ë˜ì§€ë§Œ, ë¬¸ì¥ì€ í•˜ë‚˜ë§Œ.
- ë”°ì˜´í‘œ( ", ã€ ã€ ë“±)ëŠ” ì“°ì§€ ë§ˆì„¸ìš”.

ì¶œë ¥:
- ì¡°ê±´ì„ ì§€í‚¤ëŠ” í•œêµ­ì–´ í•œ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

    resp = client.responses.create(
        model=TEXT_MODEL,
        input=[
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ],
        max_output_tokens=50,
        temperature=0.7  # ë„ˆë¬´ íŠ€ì§€ ì•Šê²Œ ì•½ê°„ ë‚®ì¶¤
    )
    return _clean_line(resp.output_text)


def generate_first_dialogue_line(char_a: dict, bg_info: dict) -> str:
    """
    ê°™ì€ ë°°ê²½/ì¸í„°ë™ì…˜ì—ì„œ char_aê°€ ë¨¼ì € í•œ ë§ˆë””ë¥¼ ìƒì„±.
    â†’ ì§§ê³  êµ¬ì–´ì²´.
    Avoid any narration or book-style phrases. The line must sound like spontaneous spoken Korean, not a written script.
    Add small hesitations (ì˜ˆ: 'ì•„...', 'ìŒ...') when appropriate, only if it fits the character.
    """
    place = bg_info.get("background", "")
    action = bg_info.get("interaction", "")
    profile = get_interaction_profile(bg_info)
    emotion_list = "\n".join([f"  - {e}" for e in profile['emotion_options']])

    char_a_data = CHARACTERS.get(char_a['book_code'], {}).get(char_a['role_key'], {})
    char_a_speech = char_a_data.get('speech_patterns', {})
    char_a_style = char_a_speech.get('speaking_style', '')
    
    system_a = (
        "ë‹¹ì‹ ì€ í•œêµ­ ì˜›ì´ì•¼ê¸° ì† ë“±ì¥ì¸ë¬¼ì´ ì‹¤ì œë¡œ ë§í•˜ëŠ” ëŒ€ì‚¬ë¥¼ ì“°ëŠ” ì‘ê°€ì…ë‹ˆë‹¤. "
        "ì²« ë²ˆì§¸ ì¸ë¬¼ì´ íˆ­ ë‚´ë±‰ëŠ” ì§§ì€ í•œ ë§ˆë””ë¥¼ ë§Œë“œì„¸ìš”."
    )
    user_a = f"""
ë°°ê²½ ì¥ì†Œ: {place}
ë°°ê²½ ì¸í„°ë™ì…˜: {action}

ì¥ë©´ ë¶„ìœ„ê¸°:
- ìš”ì•½: {profile['summary']}
- ê°€ëŠ¥í•œ ê°ì •ë“¤ (ìºë¦­í„° ì„±ê²©ì— ë§ëŠ” ê²ƒì„ ì„ íƒí•˜ì„¸ìš”):
{emotion_list}

ì²« ë²ˆì§¸ ì¸ë¬¼ ì„¤ì •(ì˜ì–´): {char_a['personality']}
ì²« ë²ˆì§¸ ì¸ë¬¼ ì •ë³´: {char_a['age']}ì‚´ {char_a['gender']}
ì²« ë²ˆì§¸ ì¸ë¬¼ ë§íˆ¬ ìŠ¤íƒ€ì¼: {char_a_style}

ìƒí™©:
- ì²« ë²ˆì§¸ ì¸ë¬¼ì´ '{action}' ì¥ë©´ ì†ì—ì„œ ìœ„ ê°ì • ì¤‘ ìì‹ ì˜ ì„±ê²©ì— ë§ëŠ” ê²ƒì„ ëŠë¼ë©° ì§§ê²Œ ë§í•©ë‹ˆë‹¤.
- ë°°ê²½ ì¥ì†Œì™€ ì¸í„°ë™ì…˜ì„ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê³  ë§¥ë½ì— ë§ëŠ” ëŒ€ì‚¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
- ì˜ˆë¥¼ ë“¤ì–´, í† ë¼ ê°™ì€ ì¥ë‚œê¾¸ëŸ¬ê¸° ìºë¦­í„°ëŠ” 'ê³ ë°±í•  ê¸°íšŒ' ê°™ì€ ì´ìƒí•œ í‘œí˜„ì„ ì“°ì§€ ë§ê³ , 
  í˜„ì¬ ìƒí™©ê³¼ ë°°ê²½ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”.

ë§íˆ¬ ê·œì¹™:
- ë¬¸ì–´ì²´(ì˜ˆ: '~ê²ƒì´ë‹¤', '~í•©ë‹ˆë‹¤') ëŒ€ì‹  ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
- ìºë¦­í„°ì˜ ë§íˆ¬ ìŠ¤íƒ€ì¼ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”. íŠ¹íˆ '~ì´ê¸°ì•¼' ê°™ì€ ë¹„ë¬¸ë²•ì  í‘œí˜„ì„ ì“°ì§€ ë§ê³  '~ì´ì§€' ê°™ì€ ì˜¬ë°”ë¥¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- '~ì´ê¸°ì—ìš”' ê°™ì€ ë¹„ë¬¸ë²•ì  í‘œí˜„ì„ ì“°ì§€ ë§ê³  '~ì´ì—ìš”' ê°™ì€ ì˜¬ë°”ë¥¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- ë§¥ë½ì— ë§ì§€ ì•ŠëŠ” ì´ìƒí•œ í‘œí˜„(ì˜ˆ: 'ê³ ë°±í•  ê¸°íšŒ', 'ì•„ë²„ì§€í•œí…Œ ê³ ë°±' ë“±)ì„ í”¼í•˜ê³ , 
  í˜„ì¬ ë°°ê²½ê³¼ ì¸í„°ë™ì…˜ì— ìì—°ìŠ¤ëŸ½ê²Œ ì–´ìš¸ë¦¬ëŠ” ëŒ€ì‚¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
- ìµœëŒ€í•œ ì§§ê³  ê°„ë‹¨í•˜ê²Œ, ì¼ìƒ ëŒ€í™”ì²˜ëŸ¼. (ì˜ˆ: '~í• ê¹Œ?', '~í•˜ëŠ” ê±°ì§€', '~ê°™ì€ë°' ë“±)
- í•œ ë¬¸ì¥ë§Œ, 1~2ì´ˆì— ë§í•  ìˆ˜ ìˆëŠ” ê¸¸ì´.
- ë”°ì˜´í‘œëŠ” ì“°ì§€ ë§ˆì„¸ìš”.
"""
    resp_a = client.responses.create(
        model=TEXT_MODEL,
        input=[
            {"role": "system", "content": system_a},
            {"role": "user", "content": user_a}
        ],
        max_output_tokens=50,
        temperature=0.7
    )
    line_a = _clean_line(resp_a.output_text)
    return line_a


def generate_second_dialogue_line(char_b: dict, line_a: str, bg_info: dict) -> str:
    """
    char_bê°€ char_aì˜ ë§(line_a)ì— ë°˜ì‘í•˜ëŠ” í•œ ë§ˆë””ë¥¼ ìƒì„±.
    â†’ ì§§ê³  êµ¬ì–´ì²´.
    """
    place = bg_info.get("background", "")
    action = bg_info.get("interaction", "")
    profile = get_interaction_profile(bg_info)
    emotion_list = "\n".join([f"  - {e}" for e in profile['emotion_options']])

    char_b_data = CHARACTERS.get(char_b['book_code'], {}).get(char_b['role_key'], {})
    char_b_speech = char_b_data.get('speech_patterns', {})
    char_b_style = char_b_speech.get('speaking_style', '')
    
    system_b = (
        "ë‹¹ì‹ ì€ í•œêµ­ ì˜›ì´ì•¼ê¸° ì† ë‘ ì¸ë¬¼ì´ ì‹¤ì œë¡œ ì£¼ê³ ë°›ëŠ” ëŒ€í™”ë¥¼ ì“°ëŠ” ì‘ê°€ì…ë‹ˆë‹¤. "
        "ë‘ ë²ˆì§¸ ì¸ë¬¼ì´ ì²« ë²ˆì§¸ ì¸ë¬¼ì˜ ë§ì„ ë“£ê³  ì§ì ‘ì ìœ¼ë¡œ ë°˜ì‘í•˜ëŠ” ì§§ì€ í•œ ë§ˆë””ë¥¼ ë§Œë“œì„¸ìš”. "
        "ë°˜ë“œì‹œ ì²« ë²ˆì§¸ ì¸ë¬¼ì˜ ë§ì— ëŒ€í•œ ì‘ë‹µì´ì–´ì•¼ í•˜ë©°, í˜¼ì£ë§ì´ ì•„ë‹Œ ëŒ€í™”ì—¬ì•¼ í•©ë‹ˆë‹¤."
    )
    user_b = f"""
ë°°ê²½ ì¥ì†Œ: {place}
ë°°ê²½ ì¸í„°ë™ì…˜: {action}
ì¥ë©´ ë¶„ìœ„ê¸° ìš”ì•½: {profile['summary']}
ê°€ëŠ¥í•œ ê°ì •ë“¤ (ìºë¦­í„° ì„±ê²©ì— ë§ëŠ” ê²ƒì„ ì„ íƒí•˜ì„¸ìš”):
{emotion_list}

ì²« ë²ˆì§¸ ì¸ë¬¼ì˜ ë§:
"{line_a}"

ë‘ ë²ˆì§¸ ì¸ë¬¼ ì„¤ì •(ì˜ì–´): {char_b['personality']}
ë‘ ë²ˆì§¸ ì¸ë¬¼ ì •ë³´: {char_b['age']}ì‚´ {char_b['gender']}
ë‘ ë²ˆì§¸ ì¸ë¬¼ ë§íˆ¬ ìŠ¤íƒ€ì¼: {char_b_style}

ì¤‘ìš”í•œ ìƒí™©:
- ë‘ ë²ˆì§¸ ì¸ë¬¼ì€ ìœ„ì˜ ì²« ë²ˆì§¸ ì¸ë¬¼ì˜ ë§ì„ ì§ì ‘ ë“£ê³  ìˆìŠµë‹ˆë‹¤.
- ì²« ë²ˆì§¸ ì¸ë¬¼ì˜ ë§ì— ëŒ€í•´ ë°˜ì‘í•˜ëŠ” ëŒ€ë‹µì„ í•´ì•¼ í•©ë‹ˆë‹¤.
- í˜¼ì£ë§ì´ ì•„ë‹ˆë¼ ì²« ë²ˆì§¸ ì¸ë¬¼ì—ê²Œ ë§í•˜ëŠ” ëŒ€í™”ì—¬ì•¼ í•©ë‹ˆë‹¤.
- ì²« ë²ˆì§¸ ì¸ë¬¼ì˜ ë§ì˜ ë‚´ìš©, í†¤, ì˜ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ì ì ˆíˆ ë°˜ì‘í•˜ì„¸ìš”.
- ë™ì˜, ë°˜ë°•, ì§ˆë¬¸, ì œì•ˆ, ë†€ëŒ ë“± ì²« ë²ˆì§¸ ì¸ë¬¼ì˜ ë§ì— ëŒ€í•œ ìì—°ìŠ¤ëŸ¬ìš´ ë°˜ì‘ì„ ë³´ì—¬ì£¼ì„¸ìš”.

ë§íˆ¬ ê·œì¹™:
- ì²« ë²ˆì§¸ ì¸ë¬¼ì˜ ë§ì— ì§ì ‘ì ìœ¼ë¡œ ë°˜ì‘í•˜ëŠ” ëŒ€ë‹µì´ì–´ì•¼ í•©ë‹ˆë‹¤.
- ì²« ë²ˆì§¸ ì¸ë¬¼ì˜ ë§ì˜ ë‚´ìš©ì„ ì–¸ê¸‰í•˜ê±°ë‚˜ ì°¸ì¡°í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
- ì˜ˆ: ì²« ë²ˆì§¸ê°€ "~í• ê¹Œ?"ë¼ê³  ë¬¼ìœ¼ë©´ â†’ "ê·¸ë˜, í•´ë³´ì" / "ì•ˆ ë¼" / "~í•˜ëŠ” ê²Œ ì¢‹ê² ì–´" ë“±
- ì˜ˆ: ì²« ë²ˆì§¸ê°€ "~í•´ì•¼ í•´"ë¼ê³  ë§í•˜ë©´ â†’ "ë§ì•„" / "ê·¸ë ‡ì§€ ì•Šì•„" / "~í•˜ëŠ” ê²Œ ë‚˜ì„ ê²ƒ ê°™ì€ë°" ë“±
- ì˜ˆ: ì²« ë²ˆì§¸ê°€ "~í–ˆì–´"ë¼ê³  ë§í•˜ë©´ â†’ "ì •ë§?" / "ê·¸ë˜?" / "~í–ˆêµ¬ë‚˜" ë“±
- ë¬¸ì–´ì²´ ê¸ˆì§€, ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ë§Œ. (ì˜ˆ: '~ì§€?', '~ì–ì•„', '~ë¼ë‹ˆê¹Œ', '~í•´ìš”' ë“±)
- ìºë¦­í„°ì˜ ë§íˆ¬ ìŠ¤íƒ€ì¼ì„ ì •í™•íˆ ë”°ë¥´ì„¸ìš”. íŠ¹íˆ '~ì´ê¸°ì•¼' ê°™ì€ ë¹„ë¬¸ë²•ì  í‘œí˜„ì„ ì“°ì§€ ë§ê³  '~ì´ì§€' ê°™ì€ ì˜¬ë°”ë¥¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- '~ì´ê¸°ì—ìš”' ê°™ì€ ë¹„ë¬¸ë²•ì  í‘œí˜„ì„ ì“°ì§€ ë§ê³  '~ì´ì—ìš”' ê°™ì€ ì˜¬ë°”ë¥¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- í•œ ë¬¸ì¥ë§Œ, ì§§ê²Œ.
- ë”°ì˜´í‘œëŠ” ì“°ì§€ ë§ˆì„¸ìš”.
"""
    resp_b = client.responses.create(
        model=TEXT_MODEL,
        input=[
            {"role": "system", "content": system_b},
            {"role": "user", "content": user_b}
        ],
        max_output_tokens=50,
        temperature=0.7
    )
    line_b = _clean_line(resp_b.output_text)
    return line_b


def generate_dialogue_lines(char_a: dict, char_b: dict, bg_info: dict) -> tuple[str, str]:
    """
    ê°™ì€ ë°°ê²½/ì¸í„°ë™ì…˜ì—ì„œ char_aê°€ ë¨¼ì € í•œ ë§ˆë””,
    char_bê°€ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ í•œ ë§ˆë””.
    â†’ ë‘˜ ë‹¤ ì§§ê³  êµ¬ì–´ì²´.
    (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€, í•˜ì§€ë§Œ ìˆœì°¨ ìƒì„±/ì¬ìƒì„ ìœ„í•´ generate_first_dialogue_lineê³¼ generate_second_dialogue_lineì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥)
    """
    line_a = generate_first_dialogue_line(char_a, bg_info)
    line_b = generate_second_dialogue_line(char_b, line_a, bg_info)
    return line_a, line_b


def generate_surprised_line(character: dict, bg_info: dict) -> str:
    """
    ë°°ê²½ì´ ê°‘ìê¸° ë°”ë€Œì—ˆì„ ë•Œ ë†€ë¼ëŠ” í•œ ë§ˆë””.
    â†’ ê°íƒ„ + ì§§ì€ êµ¬ì–´ì²´.
    """
    place = bg_info.get("background", "")
    action = bg_info.get("interaction", "")
    profile = get_interaction_profile(bg_info)
    emotion_list = "\n".join([f"  - {e}" for e in profile['emotion_options']])

    system = (
        "ë‹¹ì‹ ì€ í•œêµ­ ì˜›ì´ì•¼ê¸° ì† ë“±ì¥ì¸ë¬¼ì´ ê°‘ìê¸° ë‹¤ë¥¸ ì¥ì†Œë¡œ ì´ë™í–ˆì„ ë•Œì˜ ë°˜ì‘ì„ ì“°ëŠ” ì‘ê°€ì…ë‹ˆë‹¤. "
        "ì‹¤ì œ ì‚¬ëŒì´ ë†€ë¼ì„œ íˆ­ ë‚´ë±‰ëŠ” ì§§ì€ í•œêµ­ì–´ í•œ ë§ˆë””ë¥¼ ë§Œë“œì„¸ìš”."
    )

    # ìºë¦­í„°ì˜ speech_patterns ê°€ì ¸ì˜¤ê¸°
    char_data = CHARACTERS.get(character['book_code'], {}).get(character['role_key'], {})
    speech_patterns = char_data.get('speech_patterns', {})
    frequent_expressions = speech_patterns.get('frequent_expressions', [])
    speaking_style = speech_patterns.get('speaking_style', '')
    
    user = f"""
ìƒˆ ë°°ê²½ ì¥ì†Œ: {place}
ìƒˆ ë°°ê²½ ì¸í„°ë™ì…˜: {action}
ì¥ë©´ ë¶„ìœ„ê¸° ìš”ì•½: {profile['summary']}
ê°€ëŠ¥í•œ ê°ì •ë“¤ (ìºë¦­í„° ì„±ê²©ì— ë§ëŠ” ê²ƒì„ ì„ íƒí•˜ì„¸ìš”):
{emotion_list}

ìºë¦­í„° ì„¤ì •(ì˜ì–´): {character['personality']}
ìºë¦­í„° ì •ë³´: {character['age']}ì‚´ {character['gender']}
ìºë¦­í„° ë§íˆ¬ ìŠ¤íƒ€ì¼: {speaking_style}

ìƒí™©:
- ì´ ìºë¦­í„°ëŠ” ë°©ê¸ˆ ì „ê¹Œì§€ ì „í˜€ ë‹¤ë¥¸ ê³³ì— ìˆì—ˆëŠ”ë°,
  ê°‘ìê¸° ì´ ì¥ë©´ìœ¼ë¡œ ìˆœê°„ì´ë™í•˜ë“¯ ì˜®ê²¨ì¡ŒìŠµë‹ˆë‹¤.
- ìœ„ ê°ì • ì¤‘ ìì‹ ì˜ ì„±ê²©ì— ë§ëŠ” ê²ƒì„ ëŠë¼ë©°, ë†€ë¼ê±°ë‚˜ ë‹¹í™©í•˜ê±°ë‚˜ ì‹ ê¸°í•´ì„œ ê°íƒ„ê³¼ í•¨ê»˜ í•œ ë§ˆë””ë¥¼ í•©ë‹ˆë‹¤.
- ë°°ê²½ ì¥ì†Œì™€ ì¸í„°ë™ì…˜ì„ íŒŒì•…í•˜ê³ , ì´ê³³ì—ì„œ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ì´í•´í•œ í›„ ë†€ë¼ì›€ì„ í‘œí˜„í•©ë‹ˆë‹¤.

ë§íˆ¬ ê·œì¹™:
- ì´ ìºë¦­í„°ì˜ ì„±ê²©ê³¼ ë§íˆ¬ ìŠ¤íƒ€ì¼ì— ë§ëŠ” êµ¬ì²´ì ì¸ ê°íƒ„ì‚¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
  ì˜ˆ: ê²ë§ì€ ìºë¦­í„°ëŠ” 'ì–´? ì—¬ê¸°ê°€ ì–´ë””ì§€?', 'ë¬´ì„œìš´ ê³³ì´ë„¤...', 'ì´ìƒí•œ ê³³ì— ì™”ì–´' ë“±
      ìš©ê°í•œ ìºë¦­í„°ëŠ” 'ì˜¤? ì´ê³³ì´ ë°”ë¡œ ê·¸ ê³³ì¸ê°€?', 'í , ì—¬ê¸°ì„œ ë­˜ í•´ì•¼ í•˜ì§€?', 'ë­ì§€, ì´ ë¶„ìœ„ê¸°ëŠ”?' ë“±
      ì¥ë‚œê¾¸ëŸ¬ê¸°ëŠ” 'ì–´? ì´ê±° ì¬ë°Œê² ëŠ”ë°!', 'ì˜¤í˜¸, ì—¬ê¸°ì„œ ë­˜ í•  ìˆ˜ ìˆì„ê¹Œ?', 'ì´ëŸ° ê³³ì´ ìˆì—ˆêµ¬ë‚˜!' ë“±
      ì°¨ë¶„í•œ ìºë¦­í„°ëŠ” 'ì–´ë¼, ì—¬ê¸°ê°€ ì–´ë””ì¼ê¹Œ?', 'ì´ìƒí•˜ë„¤, ë¶„ìœ„ê¸°ê°€ ë‹¬ë¼', 'ìŒ... ì´ê³³ì€ ë­”ê°€ íŠ¹ë³„í•´' ë“±
- 'ì–´ë¼, ì´ê²Œ ë¬´ìŠ¨ ì‹ ê¸°í•œ ì¼ì¸ê°€?', 'ì˜¤í˜¸, ì´ê±° ì¬ë°Œë„¤!' ê°™ì€ ì¼ë°˜ì ì¸ ë©˜íŠ¸ëŠ” í”¼í•˜ê³ , 
  í˜„ì¬ ë°°ê²½ ì¥ì†Œì™€ ì¸í„°ë™ì…˜ì„ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ëŠ” ë†€ë¼ì›€ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- ë°°ê²½ ì¥ì†Œë‚˜ ì¸í„°ë™ì…˜ì„ ì–¸ê¸‰í•˜ë©´ì„œ ë†€ë¼ì›€ì„ í‘œí˜„í•˜ì„¸ìš”.
- ë¬¸ì–´ì²´ ê¸ˆì§€, ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´.
- í•œ ë¬¸ì¥, ì ë‹¹í•œ ê¸¸ì´ (1~2ì´ˆì— ë§í•  ìˆ˜ ìˆëŠ” ê¸¸ì´).
- ë”°ì˜´í‘œëŠ” ì“°ì§€ ë§ˆì„¸ìš”.
"""

    resp = client.responses.create(
        model=TEXT_MODEL,
        input=[
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ],
        max_output_tokens=40,
        temperature=0.7
    )
    return _clean_line(resp.output_text)


def generate_sisters_two_lines(sisters: dict, bg_info: dict) -> tuple[str, str]:
    place = bg_info.get("background", "")
    action = bg_info.get("interaction", "")
    profile = get_interaction_profile(bg_info)
    emotion_list = "\n".join([f"  - {e}" for e in profile['emotion_options']])

    system = (
        "ë‹¹ì‹ ì€ í•œêµ­ ì˜›ì´ì•¼ê¸° 'ì¥í™”í™ë ¨ì „'ì˜ ìë§¤ê°€ ì‹¤ì œë¡œ ì£¼ê³ ë°›ëŠ” ëŒ€ì‚¬ë¥¼ ì“°ëŠ” ì‘ê°€ì…ë‹ˆë‹¤. "
        "ì–¸ë‹ˆì™€ ë™ìƒì´ ì„œë¡œì—ê²Œ í•˜ëŠ” ì§§ì€ êµ¬ì–´ì²´ í•œ ë§ˆë””ì”©, ë‘ ë¬¸ì¥ì„ ë§Œë“œì„¸ìš”."
    )

    user = f"""
ë°°ê²½ ì¥ì†Œ: {place}
ë°°ê²½ ì¸í„°ë™ì…˜: {action}

ì¥ë©´ ë¶„ìœ„ê¸°:
- ìš”ì•½: {profile['summary']}
- ê°€ëŠ¥í•œ ê°ì •ë“¤ (ê° ìë§¤ì˜ ì„±ê²©ì— ë§ëŠ” ê²ƒì„ ì„ íƒí•˜ì„¸ìš”):
{emotion_list}

ìë§¤ ì„¤ì •(ì˜ì–´): {sisters['personality']}
ìë§¤ ì •ë³´: {sisters['age']}ì‚´ {sisters['gender']}

ì¶œë ¥ ê·œì¹™:
- ì²« ë²ˆì§¸ ì¤„: ì–¸ë‹ˆê°€ ë™ìƒì—ê²Œ ë§í•©ë‹ˆë‹¤. ë°˜ë“œì‹œ 'í™ë ¨ì•„' í¬í•¨.
- ë‘ ë²ˆì§¸ ì¤„: ë™ìƒì´ ì–¸ë‹ˆì—ê²Œ ë§í•©ë‹ˆë‹¤. ë°˜ë“œì‹œ 'ì–¸ë‹ˆ' í¬í•¨.
- ë‘ ë¬¸ì¥ ëª¨ë‘ ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ì—¬ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: '~ê±°ì•¼', '~í•˜ì§€ ë§ˆ', '~í•´ë³¼ê¹Œ' ë“±)
- íŠ¹íˆ '~ì´ê¸°ì•¼' ê°™ì€ ë¹„ë¬¸ë²•ì  í‘œí˜„ì„ ì“°ì§€ ë§ê³  '~ì´ì§€' ê°™ì€ ì˜¬ë°”ë¥¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
- ë¬¸ì–´ì²´ ê¸ˆì§€, ì„¤ëª… ê¸ˆì§€.
- ê°ê° í•œ ë¬¸ì¥ì”©ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

    resp = client.responses.create(
        model=TEXT_MODEL,
        input=[
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ],
        max_output_tokens=80,
        temperature=0.7
    )

    lines = [l.strip() for l in resp.output_text.splitlines() if l.strip()]
    if len(lines) >= 2:
        return _clean_line(lines[0]), _clean_line(lines[1])
    elif len(lines) == 1:
        return _clean_line(lines[0]), "ì–¸ë‹ˆ, ë‚˜ë„ ê·¸ëŸ° ê¸°ë¶„ì´ì•¼."
    else:
        return "í™ë ¨ì•„, ë„ˆë¬´ ê±±ì •í•˜ì§€ ë§ˆ.", "ì–¸ë‹ˆ, ê·¸ë˜ë„ ì¢€ ë¬´ì„œì›Œ."


# ============================================
# 5. TTS
# ============================================
def generate_tts(character: dict, text: str, output_path: str):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    speaker_tag = f"{character['book_code'].upper()}-{character['role_key'].upper()}"
    
    # ì˜ì–´ ë²ˆì—­ ìƒì„±
    try:
        translation_resp = client.responses.create(
            model=TEXT_MODEL,
            input=[
                {"role": "system", "content": "You are a translator. Translate the given Korean dialogue to natural English, preserving the character's tone and emotion."},
                {"role": "user", "content": f"Translate this Korean dialogue to English: {text}"}
            ],
            max_output_tokens=50,
            temperature=0.3
        )
        english_text = _clean_line(translation_resp.output_text)
        print(f"ğŸ¤ [{speaker_tag}] line: {text} | {english_text}")
    except Exception as e:
        print(f"ğŸ¤ [{speaker_tag}] line: {text}")
        print(f"âš ï¸ Translation failed: {e}")

    voice_speed = character.get("speed", 1.0)

    response = client.audio.speech.create(
        model=TTS_MODEL,
        voice=character["voice"],
        input=text,
        response_format="wav",
        speed=voice_speed
    )

    audio_bytes = response.read()

    # ì„ì‹œ íŒŒì¼ì— ì›ë³¸ ì˜¤ë””ì˜¤ ì €ì¥
    import tempfile
    temp_dir = tempfile.gettempdir()
    temp_input = os.path.join(temp_dir, f"tts_temp_{os.getpid()}_{id(character)}.wav")
    with open(temp_input, "wb") as f:
        f.write(audio_bytes)
    
    # íŠ¹ìˆ˜ ìºë¦­í„° ì˜¤ë””ì˜¤ íš¨ê³¼ ì ìš©
    book_code = character.get("book_code", "")
    role_key = character.get("role_key", "")
    
    if (book_code == "JHHRJ" and role_key == "ghost") or (book_code == "KWJ" and role_key == "monster"):
        # reverb íš¨ê³¼ ì ìš© (aecho í•„í„° ì‚¬ìš©)
        # ghostì˜ ê²½ìš°: ë” ì„œê¸€í”„ê³  ìš¸ë¨¹ê±°ë¦¬ëŠ” íš¨ê³¼ë¥¼ ìœ„í•´ tremoloì™€ pitch ì¡°ì •ë„ ì¶”ê°€
        if book_code == "JHHRJ" and role_key == "ghost":
            # ghost: êµ¬ìŠ¬í”„ê³  ìš°ìš¸í•˜ê³  í•œì´ ì„œë¦° ì²˜ë…€ê·€ì‹  ëª©ì†Œë¦¬
            # íš¨ê³¼: ê¹Šì€ reverb + ê°•í•œ tremolo (ìš¸ë¨¹ê±°ë¦¼) + ë‚®ì€ pitch (ì–´ë‘¡ê³  ìš°ìš¸) + ëŠë¦° ì†ë„ + ê³ ì£¼íŒŒ í•„í„°ë§ + delay + equalizer
            audio_filter = (
                "asetrate=44100*0.92,aresample=44100,"
                "atempo=0.95,"
                "lowpass=f=3000,"
                "aecho=1.0:0.9:120:0.5,"
                "adelay=50|50,"
                "tremolo=f=3.0:d=0.4,"
                "equalizer=f=200:width_type=h:width=300:g=2,"
                "equalizer=f=5000:width_type=h:width=2000:g=-3"
            )
            subprocess.run(
                ["ffmpeg", "-y", "-i", temp_input,
                 "-af", audio_filter,
                 output_path],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                check=True
            )
        else:
            # monster: reverbë§Œ ì ìš©
            subprocess.run(
                ["ffmpeg", "-y", "-i", temp_input,
                 "-af", "aecho=0.8:0.88:60:0.4",
                 output_path],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                check=True
            )
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            os.remove(temp_input)
        except:
            pass
    elif book_code == "SCJ" and role_key == "simcheong":
        # ì‹¬ì²­: ì–´ë¦¬ê³  ëª…ë‘í•˜ê³  ê²°ì—°ì— ê°€ë“ ì°¬ ëª©ì†Œë¦¬
        # íš¨ê³¼: ë†’ì€ pitch (ì–´ë¦¬ê³  ë°ê²Œ) + ë¹ ë¥¸ ì†ë„ (ëª…ë‘í•¨) + ê³ ì£¼íŒŒ ê°•ì¡° (ë§‘ê³  ë°ê²Œ) + vibrato (ìƒë™ê°) + ì €ì£¼íŒŒ ì–µì œ (ê°€ë³ê³  ë°ê²Œ)
        # ì˜¤ë””ì˜¤ í•„í„° ì²´ì¸ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
        audio_filter = (
            "asetrate=44100*1.12,aresample=44100,"  # pitch ì˜¬ë¦¼ (ë” ì–´ë¦¬ê³  ë°ê²Œ)
            "atempo=1.08,"  # ì†ë„ ë¹ ë¥´ê²Œ (ëª…ë‘í•˜ê³  í™œê¸°ì°¨ê²Œ)
            "equalizer=f=3000:width_type=h:width=2000:g=3,"  # ê³ ì£¼íŒŒ ê°•ì¡° (ë§‘ê³  ë°ê²Œ)
            "equalizer=f=5000:width_type=h:width=1500:g=2,"  # ë” ë†’ì€ ê³ ì£¼íŒŒ ê°•ì¡° (ëª…ë‘í•¨)
            "equalizer=f=200:width_type=h:width=300:g=-2,"  # ì €ì£¼íŒŒ ì–µì œ (ê°€ë³ê³  ë°ê²Œ)
            "vibrato=f=5.5:d=0.15,"  # ì•½ê°„ì˜ vibrato (ìƒë™ê°ê³¼ ê²°ì—°í•¨)
            "highpass=f=100"  # ë§¤ìš° ë‚®ì€ ì£¼íŒŒìˆ˜ ì œê±° (ë” ë§‘ê²Œ)
        )
        subprocess.run(
            ["ffmpeg", "-y", "-i", temp_input,
             "-af", audio_filter,
             output_path],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            check=True
        )
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            os.remove(temp_input)
        except:
            pass
    elif book_code == "DGJ":
        if role_key == "fox":
            # ì—¬ìš°: êµí™œí•˜ê³  ê°€ëŠ” ëª©ì†Œë¦¬, ê°„ì‹ ë°° ëŠë‚Œ
            # pitchë¥¼ ì•½ê°„ ì˜¬ë ¤ì„œ ë” ê°€ëŠ˜ê²Œ, tremoloë¥¼ ì•½ê°„ ì¶”ê°€í•´ì„œ êµí™œí•œ ëŠë‚Œ
            subprocess.run(
                ["ffmpeg", "-y", "-i", temp_input,
                 "-af", "asetrate=44100*1.15,aresample=44100,tremolo=f=3.0:d=0.2",
                 output_path],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                check=True
            )
        elif role_key == "toad":
            # ë‘êº¼ë¹„: í˜„ëª…í•˜ê³  ì´ëª…í•˜ê³  ë­‰íˆ­í•˜ê³  ë¬µì§í•œ ëª©ì†Œë¦¬
            # pitchë¥¼ ì•½ê°„ ë‚®ì¶°ì„œ ë” ë¬µì§í•˜ê²Œ, bass boostë¡œ ë” ê¹Šê³  ë­‰íˆ­í•œ ëŠë‚Œ
            subprocess.run(
                ["ffmpeg", "-y", "-i", temp_input,
                 "-af", "asetrate=44100*0.9,aresample=44100,equalizer=f=100:width_type=h:width=200:g=3",
                 output_path],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                check=True
            )
        else:
            # ë‹¤ë¥¸ DGJ ìºë¦­í„°ëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ
            with open(output_path, "wb") as f:
                f.write(audio_bytes)
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            os.remove(temp_input)
        except:
            pass
    else:
        # ì¼ë°˜ ìºë¦­í„°ëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ ì €ì¥
        with open(output_path, "wb") as f:
            f.write(audio_bytes)
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            os.remove(temp_input)
        except:
            pass
    
    print(f"âœ… Saved: {output_path}")

    return output_path



def play_audio(path: str):
    print(f"ğŸ”Š PLAY AUDIO: {path}")
    subprocess.run(["afplay", path])


# ============================================
# 6. ë©”ì¸ ì§„ì…ì : ì›¹ìº ì—ì„œ book_code + ìˆœì„œ ë„˜ê²¨ì¤„ ë•Œ
# ============================================
def handle_book_input(book_code: str, index_in_sequence: int):
    """
    index_in_sequence ê·œì¹™:

    1: ì´ˆê¸° ë°°ê²½ ì„¤ì •
    2: ì´ˆê¸° cha1 ë“±ì¥ + í•œ ì¤„ ëŒ€ì‚¬
    3: ì´ˆê¸° cha2 ë“±ì¥ + cha1/cha2 ëŒ€í™” (ê° í•œ ì¤„)

    ì´í›„ 4ë¶€í„°ëŠ” 3ê°œ ì£¼ê¸°ë¡œ ë°˜ë³µ:
    4,7,10,... : ë°°ê²½ë§Œ êµì²´ â†’ cha1/cha2 ë‘˜ ë‹¤ ë†€ë¼ëŠ” ëŒ€ì‚¬ í•œ ì¤„ì”©
    5,8,11,... : cha1 êµì²´     â†’ ìƒˆ cha1 + ê¸°ì¡´ cha2 ëŒ€í™” (ê° í•œ ì¤„)
                 (ë‹¨, ìƒˆ cha1ì´ ìë§¤ë©´ ì–¸ë‹ˆ/ë™ìƒ ë‘ ì¤„ + cha2 í•œ ì¤„)
    6,9,12,... : cha2 êµì²´     â†’ ê¸°ì¡´ cha1 + ìƒˆ cha2 ëŒ€í™” (ê° í•œ ì¤„)
    """
    global CURRENT_BG_BOOK_CODE, CURRENT_BG_INFO, CURRENT_CHA1_INFO, CURRENT_CHA2_INFO

    print("\n==============================")
    print(f"[handle_book_input] book_code={book_code}, index={index_in_sequence}")
    

    # -------------------------
    # 1) index 1: ì´ˆê¸° ë°°ê²½
    # -------------------------
    if index_in_sequence == 1:
        bg = get_background(book_code)
        if bg is None:
            print(f"âš  BACKGROUNDSì— ì—†ëŠ” book_code: {book_code}")
            return

        CURRENT_BG_BOOK_CODE = book_code
        CURRENT_BG_INFO = bg
        CURRENT_CHA1_INFO = None
        CURRENT_CHA2_INFO = None

        print(f"[BACKGROUND INIT] {book_code} â†’ {bg.get('background')}")
        play_background_video(book_code)  # ë°°ê²½ ë¹„ë””ì˜¤ ì¬ìƒ (ë¬´í•œ ë£¨í”„, ì˜¤ë””ì˜¤ í¬í•¨)
        
        # ë°°ê²½ì´ ë°”ë€” ë•Œ ì‚¬ìš´ë“œ ì´í™íŠ¸ë§Œ ì¬ìƒ (ì œëª© ë§í•˜ê¸°ëŠ” ë§ˆì»¤ ê°ì§€ ì‹œì—ë§Œ ì¬ìƒ)
        sound_effect_path = "soundeffect/ES_Dream, Harp - Epidemic Sound.wav"
        
        def play_sound():
            # ES_Dream ì‚¬ìš´ë“œ ì´í™íŠ¸ë¥¼ ìŒëŸ‰ 20%ë¡œ ì²˜ë¦¬í•œ ì„ì‹œ íŒŒì¼ ìƒì„± ë° ì¬ìƒ
            if os.path.exists(sound_effect_path):
                try:
                    os.makedirs("title_saying", exist_ok=True)
                    temp_sound = f"title_saying/temp_sound_{book_code}.wav"
                    subprocess.run(
                        ["ffmpeg", "-y", "-i", sound_effect_path,
                         "-af", "volume=0.2",
                         temp_sound],
                        stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL,
                        check=True
                    )
                    # ì‚¬ìš´ë“œ ì´í™íŠ¸ ì¬ìƒ
                    subprocess.run(["afplay", temp_sound],
                                 stdout=subprocess.DEVNULL,
                                 stderr=subprocess.DEVNULL)
                    os.remove(temp_sound)
                    print(f"ğŸ”Š ì‚¬ìš´ë“œ íš¨ê³¼ ì¬ìƒ (ìŒëŸ‰ 20%): {sound_effect_path}")
                except Exception as e:
                    print(f"âš ï¸ ì‚¬ìš´ë“œ íš¨ê³¼ ì¬ìƒ ì‹¤íŒ¨: {e}")
        
        # ë¹„ë™ê¸°ë¡œ ì¬ìƒ (ë¸”ë¡œí‚¹ ë°©ì§€)
        threading.Thread(target=play_sound, daemon=True).start()
        return

    # -------------------------
    # 2) index 2: ì´ˆê¸° cha1
    # -------------------------
    if index_in_sequence == 2:
        if book_code not in ROLE_MAP:
            print(f"âš  ROLE_MAPì— ì—†ëŠ” book_code: {book_code}")
            return
        role_key = ROLE_MAP[book_code]["cha1"]
        if role_key is None:
            print(f"âš  {book_code}ì— cha1 ì •ì˜ ì—†ìŒ")
            return

        cha1 = build_character(book_code, role_key)
        CURRENT_CHA1_INFO = cha1
        
        # ì²« ë²ˆì§¸ ì±…ì´ ì‹¬ì²­ì „(SCJ)ì´ê³  ë‘ ë²ˆì§¸ ì±…ì´ ê°ì§€ë˜ë©´ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ì„¤ì •
        if CURRENT_BG_BOOK_CODE == "SCJ":
            # inter_video/inter_bgSCJ/bgSCJ_ch1_{overlay_code}.mov íŒŒì¼ ì°¾ê¸°
            overlay_code = BOOK_TO_OVERLAY_CODE.get(book_code, book_code)
            overlay_path = f"inter_video/inter_bgSCJ/bgSCJ_ch1_{overlay_code}.mov"
            if os.path.exists(overlay_path):
                VIDEO_PLAYER.set_overlay_video(overlay_path)
                print(f"ğŸ¬ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ì„¤ì •: {overlay_path}")
            else:
                print(f"âš ï¸ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {overlay_path}")

        # ì¥í™”í™ë ¨ì „ì˜ ê²½ìš° ìë§¤ ë‘˜ ë‹¤ ë§í•˜ë„ë¡
        if book_code == "JHHRJ":
            older, younger = build_sisters_pair()
            CURRENT_CHA1_INFO = older
            CURRENT_CHA2_INFO = younger
            
            # ìë§¤ ë‘˜ ë‹¤ ëŒ€ì‚¬ ìƒì„±
            line1, line2 = generate_sisters_two_lines(older, CURRENT_BG_INFO)
            if not line1 or not line2:
                # ëŒ€ì‚¬ ìƒì„± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ëŒ€ì‚¬ ì‚¬ìš©
                line1 = "í™ë ¨ì•„, ì—¬ê¸°ê°€ ì–´ë””ì§€?"
                line2 = "ì–¸ë‹ˆ, ë‚˜ë„ ëª¨ë¥´ê² ì–´."
            
            # ëœë¤ìœ¼ë¡œ ìˆœì„œ ê²°ì •
            if random.random() < 0.5:
                # ì¥í™” ë¨¼ì €
                out1 = f"output/JHHRJ_sister_older_init_cha1.wav"
                out2 = f"output/JHHRJ_sister_younger_init_cha1.wav"
                generate_tts(older, line1, out1)
                generate_tts(younger, line2, out2)
                play_audio(out1)
                play_audio(out2)
            else:
                # í™ë ¨ ë¨¼ì €
                out1 = f"output/JHHRJ_sister_younger_init_cha1.wav"
                out2 = f"output/JHHRJ_sister_older_init_cha1.wav"
                generate_tts(younger, line2, out1)
                generate_tts(older, line1, out2)
                play_audio(out1)
                play_audio(out2)
        else:
            line = generate_action_line(cha1, CURRENT_BG_INFO)
            if not line:
                line = f"{CURRENT_BG_INFO.get('interaction', '')}, í•œë²ˆ í•´ë³¼ê¹Œ?"

            out_path = f"output/{book_code}_{role_key}_init_cha1.wav"
            generate_tts(cha1, line, out_path)
            play_audio(out_path)
        return

    # -------------------------
    # 3) index 3: ì´ˆê¸° cha2 + ëŒ€í™”
    # -------------------------
    if index_in_sequence == 3:
        if book_code not in ROLE_MAP:
            print(f"âš  ROLE_MAPì— ì—†ëŠ” book_code: {book_code}")
            return
        role_key = ROLE_MAP[book_code]["cha2"]
        if role_key is None:
            print(f"âš  {book_code}ì— cha2 ì •ì˜ ì—†ìŒ")
            return

        cha2 = build_character(book_code, role_key)
        CURRENT_CHA2_INFO = cha2
        
        # ì²« ë²ˆì§¸ ì±…ì´ ì‹¬ì²­ì „(SCJ)ì´ê³  ì„¸ ë²ˆì§¸ ì±…ì´ ê°ì§€ë˜ë©´ ch2 ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ì„¤ì •
        if CURRENT_BG_BOOK_CODE == "SCJ":
            # inter_video/inter_bgSCJ/bgSCJ_ch2_{overlay_code}.mov íŒŒì¼ ì°¾ê¸°
            overlay_code = BOOK_TO_OVERLAY_CODE.get(book_code, book_code)
            overlay_path2 = f"inter_video/inter_bgSCJ/bgSCJ_ch2_{overlay_code}.mov"
            if os.path.exists(overlay_path2):
                VIDEO_PLAYER.set_overlay_video2(overlay_path2)
                print(f"ğŸ¬ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ch2 ì„¤ì •: {overlay_path2}")
            else:
                print(f"âš ï¸ ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ch2ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {overlay_path2}")

        if CURRENT_CHA1_INFO is None:
            print("âš  cha1ì´ ì•„ì§ ì„¤ì •ë˜ì§€ ì•Šì•„ cha2ë§Œ í•œ ì¤„ ëŒ€ì‚¬")
            line2 = generate_action_line(cha2, CURRENT_BG_INFO)
            out2 = f"output/{book_code}_{role_key}_init_cha2_only.wav"
            generate_tts(cha2, line2, out2)
            play_audio(out2)
            return

        # ì¥í™”í™ë ¨ì „ì˜ ê²½ìš°: ìë§¤ê°€ ëœë¤ ìˆœì„œë¡œ ë§í•¨
        if CURRENT_CHA1_INFO['book_code'] == "JHHRJ":
            older, younger = build_sisters_pair()
            
            # ëœë¤ìœ¼ë¡œ ìˆœì„œ ê²°ì •
            if random.random() < 0.5:
                # ì¥í™” ë¨¼ì €
                line_older = generate_action_line(older, CURRENT_BG_INFO)
                out_older = f"output/JHHRJ_sister_older_init_dialog1.wav"
                generate_tts(older, line_older, out_older)
                play_audio(out_older)
                
                line_younger = generate_second_dialogue_line(younger, line_older, CURRENT_BG_INFO)
                out_younger = f"output/JHHRJ_sister_younger_init_dialog2.wav"
                generate_tts(younger, line_younger, out_younger)
                play_audio(out_younger)
            else:
                # í™ë ¨ ë¨¼ì €
                line_younger = generate_action_line(younger, CURRENT_BG_INFO)
                out_younger = f"output/JHHRJ_sister_younger_init_dialog1.wav"
                generate_tts(younger, line_younger, out_younger)
                play_audio(out_younger)
                
                line_older = generate_second_dialogue_line(older, line_younger, CURRENT_BG_INFO)
                out_older = f"output/JHHRJ_sister_older_init_dialog2.wav"
                generate_tts(older, line_older, out_older)
                play_audio(out_older)
            
            # cha2ê°€ ìë§¤ì˜ ëŒ€í™”ì— ë°˜ì‘
            line_cha2 = generate_second_dialogue_line(cha2, line_younger if random.random() < 0.5 else line_older, CURRENT_BG_INFO)
            out_cha2 = f"output/{book_code}_{role_key}_init_dialog3.wav"
            generate_tts(cha2, line_cha2, out_cha2)
            play_audio(out_cha2)
        else:
            # ìƒˆë¡œ ë“±ì¥í•˜ëŠ” cha2ê°€ ë¨¼ì € ë§í•˜ê³ , cha1ì´ ëŒ€ë‹µí•˜ë„ë¡ ìˆœì„œ ë³€ê²½
            # ì²« ë²ˆì§¸ ëŒ€í™” ìƒì„± ë° ì¬ìƒ
            line2 = generate_first_dialogue_line(cha2, CURRENT_BG_INFO)
            out2 = f"output/{book_code}_{role_key}_init_dialog1.wav"
            generate_tts(cha2, line2, out2)
            play_audio(out2)
            
            # ë‘ ë²ˆì§¸ ëŒ€í™” ìƒì„± ë° ì¬ìƒ
            line1 = generate_second_dialogue_line(CURRENT_CHA1_INFO, line2, CURRENT_BG_INFO)
            out1 = f"output/{CURRENT_CHA1_INFO['book_code']}_{CURRENT_CHA1_INFO['role_key']}_init_dialog2.wav"
            generate_tts(CURRENT_CHA1_INFO, line1, out1)
            play_audio(out1)
        return

    # -------------------------
    # 4) ì´í›„: 3ê°œ ì£¼ê¸° (ë°°ê²½ / cha1 / cha2 êµì²´)
    # -------------------------
    if CURRENT_BG_INFO is None or CURRENT_CHA1_INFO is None or CURRENT_CHA2_INFO is None:
        print("âš  ì•„ì§ ì´ˆê¸° 1~3ë²ˆ ì…‹ì—…ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    offset = (index_in_sequence - 4) % 3  # 0,1,2 ë°˜ë³µ

    # ---- 4,7,10,... : ë°°ê²½ êµì²´ + ë‘ ìºë¦­í„° ë†€ëŒ ----
    if offset == 0:
        bg = get_background(book_code)
        if bg is None:
            print(f"âš  BACKGROUNDSì— ì—†ëŠ” book_code: {book_code}")
            return

        CURRENT_BG_BOOK_CODE = book_code
        CURRENT_BG_INFO = bg

        print(f"[BACKGROUND SWAP] {book_code} â†’ {bg.get('background')}")
        # ë°°ê²½ë§Œ êµì²´í•˜ê³  ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤(ch1, ch2)ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        play_background_video(book_code)  # ë°°ê²½ ë¹„ë””ì˜¤ êµì²´ (ë¬´í•œ ë£¨í”„, ì˜¤ë””ì˜¤ í¬í•¨, í˜ì´ë“œ íš¨ê³¼)

        # ë°°ê²½ì´ ë°”ë€” ë•Œ ì‚¬ìš´ë“œ ì´í™íŠ¸ë§Œ ì¬ìƒ (ì œëª© ë§í•˜ê¸°ëŠ” ë§ˆì»¤ ê°ì§€ ì‹œì—ë§Œ ì¬ìƒ)
        sound_effect_path = "soundeffect/ES_Dream, Harp - Epidemic Sound.wav"
        
        def play_sound():
            # ES_Dream ì‚¬ìš´ë“œ ì´í™íŠ¸ë¥¼ ìŒëŸ‰ 20%ë¡œ ì²˜ë¦¬í•œ ì„ì‹œ íŒŒì¼ ìƒì„± ë° ì¬ìƒ
            if os.path.exists(sound_effect_path):
                try:
                    os.makedirs("title_saying", exist_ok=True)
                    temp_sound = f"title_saying/temp_sound_{book_code}.wav"
                    subprocess.run(
                        ["ffmpeg", "-y", "-i", sound_effect_path,
                         "-af", "volume=0.2",
                         temp_sound],
                        stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL,
                        check=True
                    )
                    # ì‚¬ìš´ë“œ ì´í™íŠ¸ ì¬ìƒ
                    subprocess.run(["afplay", temp_sound],
                                 stdout=subprocess.DEVNULL,
                                 stderr=subprocess.DEVNULL)
                    os.remove(temp_sound)
                    print(f"ğŸ”Š ì‚¬ìš´ë“œ íš¨ê³¼ ì¬ìƒ (ìŒëŸ‰ 20%): {sound_effect_path}")
                except Exception as e:
                    print(f"âš ï¸ ì‚¬ìš´ë“œ íš¨ê³¼ ì¬ìƒ ì‹¤íŒ¨: {e}")
        
        # ë¹„ë™ê¸°ë¡œ ì¬ìƒ (ë¸”ë¡œí‚¹ ë°©ì§€)
        threading.Thread(target=play_sound, daemon=True).start()

        line1 = generate_surprised_line(CURRENT_CHA1_INFO, CURRENT_BG_INFO)
        line2 = generate_surprised_line(CURRENT_CHA2_INFO, CURRENT_BG_INFO)

        out1 = f"output/{CURRENT_CHA1_INFO['book_code']}_{CURRENT_CHA1_INFO['role_key']}_surprised.wav"
        out2 = f"output/{CURRENT_CHA2_INFO['book_code']}_{CURRENT_CHA2_INFO['role_key']}_surprised.wav"
        generate_tts(CURRENT_CHA1_INFO, line1, out1)
        generate_tts(CURRENT_CHA2_INFO, line2, out2)
        play_audio(out1)
        play_audio(out2)
        return

        # ---- 5,8,11,... : cha1 êµì²´ ----
    if offset == 1:
        if book_code not in ROLE_MAP:
            print(f"âš  ROLE_MAPì— ì—†ëŠ” book_code: {book_code}")
            return
        role_key = ROLE_MAP[book_code]["cha1"]
        if role_key is None:
            print(f"âš  {book_code}ì— cha1 ì •ì˜ ì—†ìŒ")
            return

        cha1 = build_character(book_code, role_key)
        CURRENT_CHA1_INFO = cha1

        # ğŸ”¸ ì¥í™”í™ë ¨ ìë§¤ì¸ ê²½ìš°: ëœë¤ ìˆœì„œë¡œ ê°ê° í•œ ì¤„ì”© ë§í•˜ê³ ,
        #    ê¸°ì¡´ cha2(ì˜ˆ: í† ë¼, ê·€ì‹  ë“±)ê°€ í•œ ì¤„ ë” ëŒ€ë‹µ.
        if book_code == "JHHRJ" and role_key == "sister_older":
            sister_older, sister_younger = build_sisters_pair()

            # ëœë¤ìœ¼ë¡œ ìˆœì„œ ê²°ì •
            if random.random() < 0.5:
                # ì–¸ë‹ˆ â†’ ë™ìƒ ìˆœì„œ
                lineA = generate_first_dialogue_line(sister_older, CURRENT_BG_INFO)
                outA = "output/JHHRJ_sister_older_line.wav"
                generate_tts(sister_older, lineA, outA)
                play_audio(outA)
                
                lineB = generate_second_dialogue_line(sister_younger, lineA, CURRENT_BG_INFO)
                outB = "output/JHHRJ_sister_younger_line.wav"
                generate_tts(sister_younger, lineB, outB)
                play_audio(outB)
            else:
                # ë™ìƒ â†’ ì–¸ë‹ˆ ìˆœì„œ
                lineB = generate_first_dialogue_line(sister_younger, CURRENT_BG_INFO)
                outB = "output/JHHRJ_sister_younger_line.wav"
                generate_tts(sister_younger, lineB, outB)
                play_audio(outB)
                
                lineA = generate_second_dialogue_line(sister_older, lineB, CURRENT_BG_INFO)
                outA = "output/JHHRJ_sister_older_line.wav"
                generate_tts(sister_older, lineA, outA)
                play_audio(outA)
            
            # cha2ê°€ ìë§¤ì˜ ëŒ€í™”ì— ë°˜ì‘
            reply = generate_second_dialogue_line(CURRENT_CHA2_INFO, lineA if random.random() < 0.5 else lineB, CURRENT_BG_INFO)
            outC = f"output/{CURRENT_CHA2_INFO['book_code']}_{CURRENT_CHA2_INFO['role_key']}_reply_to_sisters.wav"
            generate_tts(CURRENT_CHA2_INFO, reply, outC)
            play_audio(outC)
            return

        # ğŸ”¹ ê·¸ ì™¸ ì¼ë°˜ ìºë¦­í„°: ìƒˆ cha1 + ê¸°ì¡´ cha2ê°€ í•œ ì¤„ì”© ëŒ€í™”
        # ì²« ë²ˆì§¸ ëŒ€í™” ìƒì„± ë° ì¬ìƒ
        line1 = generate_first_dialogue_line(cha1, CURRENT_BG_INFO)
        out1 = f"output/{book_code}_{role_key}_swapcha1_dialog1.wav"
        generate_tts(cha1, line1, out1)
        play_audio(out1)
        
        # ë‘ ë²ˆì§¸ ëŒ€í™” ìƒì„± ë° ì¬ìƒ
        line2 = generate_second_dialogue_line(CURRENT_CHA2_INFO, line1, CURRENT_BG_INFO)
        out2 = f"output/{CURRENT_CHA2_INFO['book_code']}_{CURRENT_CHA2_INFO['role_key']}_swapcha1_dialog2.wav"
        generate_tts(CURRENT_CHA2_INFO, line2, out2)
        play_audio(out2)
        return

    # ---- 6,9,12,... : cha2 êµì²´ ----
    if offset == 2:
        if book_code not in ROLE_MAP:
            print(f"âš  ROLE_MAPì— ì—†ëŠ” book_code: {book_code}")
            return
        role_key = ROLE_MAP[book_code]["cha2"]
        if role_key is None:
            print(f"âš  {book_code}ì— cha2 ì •ì˜ ì—†ìŒ")
            return

        cha2 = build_character(book_code, role_key)
        CURRENT_CHA2_INFO = cha2

        # cha1ì´ ì¥í™”í™ë ¨ì¸ ê²½ìš°: ìë§¤ê°€ ëœë¤ ìˆœì„œë¡œ ë§í•¨
        if CURRENT_CHA1_INFO['book_code'] == "JHHRJ":
            older, younger = build_sisters_pair()
            
            # ëœë¤ìœ¼ë¡œ ìˆœì„œ ê²°ì •
            if random.random() < 0.5:
                # ì¥í™” ë¨¼ì €
                line_older = generate_action_line(older, CURRENT_BG_INFO)
                out_older = f"output/JHHRJ_sister_older_swapcha2_dialog1.wav"
                generate_tts(older, line_older, out_older)
                play_audio(out_older)
                
                line_younger = generate_second_dialogue_line(younger, line_older, CURRENT_BG_INFO)
                out_younger = f"output/JHHRJ_sister_younger_swapcha2_dialog2.wav"
                generate_tts(younger, line_younger, out_younger)
                play_audio(out_younger)
            else:
                # í™ë ¨ ë¨¼ì €
                line_younger = generate_action_line(younger, CURRENT_BG_INFO)
                out_younger = f"output/JHHRJ_sister_younger_swapcha2_dialog1.wav"
                generate_tts(younger, line_younger, out_younger)
                play_audio(out_younger)
                
                line_older = generate_second_dialogue_line(older, line_younger, CURRENT_BG_INFO)
                out_older = f"output/JHHRJ_sister_older_swapcha2_dialog2.wav"
                generate_tts(older, line_older, out_older)
                play_audio(out_older)
            
            # cha2ê°€ ìë§¤ì˜ ëŒ€í™”ì— ë°˜ì‘
            line_cha2 = generate_second_dialogue_line(cha2, line_younger if random.random() < 0.5 else line_older, CURRENT_BG_INFO)
            out_cha2 = f"output/{book_code}_{role_key}_swapcha2_dialog3.wav"
            generate_tts(cha2, line_cha2, out_cha2)
            play_audio(out_cha2)
        else:
            # cha2ê°€ ë¨¼ì € ë§í•˜ê³ , cha1ì´ ëŒ€ë‹µí•˜ë„ë¡ ìˆœì„œ ë³€ê²½
            # ì²« ë²ˆì§¸ ëŒ€í™” ìƒì„± ë° ì¬ìƒ
            line2 = generate_first_dialogue_line(cha2, CURRENT_BG_INFO)
            out2 = f"output/{book_code}_{role_key}_swapcha2_dialog1.wav"
            generate_tts(cha2, line2, out2)
            play_audio(out2)
            
            # ë‘ ë²ˆì§¸ ëŒ€í™” ìƒì„± ë° ì¬ìƒ
            line1 = generate_second_dialogue_line(CURRENT_CHA1_INFO, line2, CURRENT_BG_INFO)
            out1 = f"output/{CURRENT_CHA1_INFO['book_code']}_{CURRENT_CHA1_INFO['role_key']}_swapcha2_dialog2.wav"
            generate_tts(CURRENT_CHA1_INFO, line1, out1)
            play_audio(out1)
        return


# ============================================
# 7. ì›¹ìº  ArUco ë§ˆì»¤ ê°ì§€
# ============================================
def run_webcam_detection():
    """
    ì›¹ìº ìœ¼ë¡œ ArUco ë§ˆì»¤ë¥¼ ê°ì§€í•˜ê³ , ê°ì§€ëœ ë§ˆì»¤ì— ë”°ë¼ handle_book_inputì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
    ë°°ê²½ ë¹„ë””ì˜¤ëŠ” ë³„ë„ì˜ ìœˆë„ìš°ì—ì„œ ë¶€ë“œëŸ½ê²Œ ì „í™˜ë˜ë©° ë¬´í•œ ë£¨í”„ë¡œ ì¬ìƒë©ë‹ˆë‹¤.
    TTS ìƒì„±ì€ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ì–´ ë¹„ë””ì˜¤ê°€ ëŠê¸°ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    global CURRENT_BG_BOOK_CODE
    
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    print("ğŸ“· Camera . Press 'q' to quit.")
    print("ğŸ“š Show your book to camera...")
    
    # ë¹„ë””ì˜¤ í”Œë ˆì´ì–´ ì‹œì‘
    VIDEO_PLAYER.start()
    
    detector_params = aruco.DetectorParameters()
    detector = aruco.ArucoDetector(ARUCO_DICTIONARY, detector_params)
    
    sequence_index = 0  # í˜„ì¬ ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤
    last_detected_marker = None  # ë§ˆì§€ë§‰ìœ¼ë¡œ ê°ì§€ëœ ë§ˆì»¤ (ì¤‘ë³µ ë°©ì§€)
    handler_thread = None  # handle_book_input ì‹¤í–‰ ìŠ¤ë ˆë“œ
    is_processing = False  # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ì§€ ì—¬ë¶€
    
    # ë¹„ë””ì˜¤ ìœˆë„ìš° ìƒì„± (íŒì—…ì°½)
    cv2.namedWindow("Background Video", cv2.WINDOW_NORMAL)
    # ì°½ í¬ê¸° ì„¤ì • (ì˜ˆ: 1280x720)
    cv2.resizeWindow("Background Video", 1280, 720)
    
    def run_handler_async(book_code, seq_idx):
        """handle_book_inputì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰"""
        nonlocal is_processing
        is_processing = True
        try:
            handle_book_input(book_code, seq_idx)
        finally:
            is_processing = False
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print("âŒ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            break
        
        # ArUco ë§ˆì»¤ ê°ì§€
        corners, ids, rejected = detector.detectMarkers(frame)
        
        # ê°ì§€ëœ ë§ˆì»¤ê°€ ìˆìœ¼ë©´ í‘œì‹œ
        if ids is not None:
            aruco.drawDetectedMarkers(frame, corners, ids)
            
            # ì²« ë²ˆì§¸ë¡œ ê°ì§€ëœ ë§ˆì»¤ ì²˜ë¦¬
            marker_id = ids[0][0]
            book_code = get_book_code_from_marker(marker_id)
            
            # ì´ì „ í•¸ë“¤ëŸ¬ê°€ ì™„ë£Œëœ ê²½ìš°ì—ë§Œ ìƒˆ ë§ˆì»¤ ì²˜ë¦¬
            if book_code and marker_id != last_detected_marker and not is_processing:
                last_detected_marker = marker_id
                sequence_index += 1
                
                # í•œê¸€ ì±… ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                book_info = BACKGROUNDS.get(book_code, {})
                book_name_kr = book_info.get("book", book_code)
                
                print(f"\nğŸ¯ Marker Detected! ID: {marker_id} â†’ {book_name_kr} ({book_code}) (Num of books: {sequence_index})")
                
                # ë§ˆì»¤ ê°ì§€ ì¦‰ì‹œ ì œëª© ë§í•˜ê¸° ì¬ìƒ (ë°°ê²½ì´ ë°”ë€” ë•Œë§Œ ì‚¬ìš´ë“œ ì´í™íŠ¸ í¬í•¨)
                title_saying_path = f"title_saying/{book_code}_title.wav"
                
                def play_title():
                    # ì œëª© ë§í•˜ê¸° ì¬ìƒ (ìŒëŸ‰ 150%)
                    if os.path.exists(title_saying_path):
                        # ìŒëŸ‰ 150%ë¡œ ì¡°ì •í•œ ì„ì‹œ íŒŒì¼ ìƒì„±
                        import tempfile
                        temp_dir = tempfile.gettempdir()
                        temp_title = os.path.join(temp_dir, f"title_{os.getpid()}_{id(title_saying_path)}.wav")
                        subprocess.run(
                            ["ffmpeg", "-y", "-i", title_saying_path,
                             "-af", "volume=1.5",
                             "-acodec", "pcm_s16le", "-ar", "44100", "-ac", "2",
                             temp_title],
                            stdout=subprocess.DEVNULL,
                            stderr=subprocess.DEVNULL,
                            check=True
                        )
                        # ì¡°ì •ëœ íŒŒì¼ ì¬ìƒ
                        subprocess.run(["afplay", temp_title],
                                      stdout=subprocess.DEVNULL,
                                      stderr=subprocess.DEVNULL)
                        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                        try:
                            os.remove(temp_title)
                        except:
                            pass
                        print(f"ğŸ“š ì œëª© ë§í•˜ê¸° ì¬ìƒ (ìŒëŸ‰ 150%): {title_saying_path}")
                    
                    # ì œëª© ë§í•˜ê¸°ê°€ ëë‚œ í›„ BGM ì¬ìƒ
                    if VIDEO_PLAYER.pending_bgm_path:
                        bgm_path = VIDEO_PLAYER.pending_bgm_path
                        VIDEO_PLAYER.pending_bgm_path = None
                        VIDEO_PLAYER._start_bgm(bgm_path)
                
                # ë¹„ë™ê¸°ë¡œ ì¬ìƒ (ë¸”ë¡œí‚¹ ë°©ì§€)
                threading.Thread(target=play_title, daemon=True).start()
                
                # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ handle_book_input ì‹¤í–‰ (ë¹„ë””ì˜¤ê°€ ëŠê¸°ì§€ ì•Šë„ë¡)
                handler_thread = threading.Thread(
                    target=run_handler_async, 
                    args=(book_code, sequence_index),
                    daemon=True
                )
                handler_thread.start()
        
        # í™”ë©´ì— ì •ë³´ í‘œì‹œ (ì›¹ìº  ìœˆë„ìš°)
        cv2.putText(frame, f"Sequence: {sequence_index}", (10, 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        if last_detected_marker is not None:
            book = get_book_code_from_marker(last_detected_marker) or "Unknown"
            cv2.putText(frame, f"Last: {book}", (10, 70), 
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        if is_processing:
            cv2.putText(frame, "Processing...", (10, 110), 
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 165, 255), 2)
        
        cv2.imshow("ArUco Marker Detection", frame)
        
        # ë°°ê²½ ë¹„ë””ì˜¤ í”„ë ˆì„ í‘œì‹œ (ê°™ì€ ìœˆë„ìš°ì—ì„œ ë¶€ë“œëŸ½ê²Œ ì „í™˜)
        video_frame = VIDEO_PLAYER.get_frame()
        if video_frame is not None:
            cv2.imshow("Background Video", video_frame)
        
        # 'q' í‚¤ë¡œ ì¢…ë£Œ
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    # ì •ë¦¬
    cap.release()
    VIDEO_PLAYER.stop()
    cv2.destroyAllWindows()
    print("\nğŸ“· ì›¹ìº  ì¢…ë£Œë¨.")


# ============================================
# 8. ë‹¨ë… ì‹¤í–‰
# ============================================
if __name__ == "__main__":
    import sys
    
    # ArUco ë§ˆì»¤ ìƒì„± ì˜µì…˜
    if len(sys.argv) > 1 and sys.argv[1] == "--generate-markers":
        generate_aruco_markers()
        sys.exit(0)
    
    # ê¸°ë³¸: ì›¹ìº  ê°ì§€ ëª¨ë“œ ì‹¤í–‰
    run_webcam_detection()